{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "VoyZ5yLRaAeI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-09-06 18:38:51.564229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-06 18:38:51.564484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-06 18:38:51.564631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-06 18:38:51.564810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-06 18:38:51.564948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-06 18:38:51.565083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 21650 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "Ah7ISzvtEmur"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import IPython.display as ipd\n",
        "import librosa.display\n",
        "import scipy\n",
        "import glob\n",
        "import numpy as np\n",
        "import math\n",
        "import warnings\n",
        "import pickle\n",
        "from sklearn.utils import shuffle\n",
        "import zipfile\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "WBZyUMpobR-Z"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-09-06 18:38:51.607013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-06 18:38:51.607254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-06 18:38:51.607393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-06 18:38:51.607562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-06 18:38:51.607699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-06 18:38:51.607831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 21650 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 15595309183850485979\n",
              " xla_global_id: -1,\n",
              " name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 22702456832\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 8268132397181295658\n",
              " physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
              " xla_global_id: 416903419]"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "zu2lgR8lEmu0"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(999)\n",
        "np.random.seed(999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "JGbkvQHxxrj_"
      },
      "outputs": [],
      "source": [
        "# !wget 'cdn.daitan.com/dataset.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "51o6Ze9hxrkE"
      },
      "outputs": [],
      "source": [
        "# dataset_file_name = './dataset.zip'\n",
        "# with zipfile.ZipFile(dataset_file_name, 'r') as zip_ref:\n",
        "#     zip_ref.extractall('./dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"cnn\"\n",
        "model_name = \"lstm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "d43UvhEqxrkJ"
      },
      "outputs": [],
      "source": [
        "# path_to_dataset = \"./dataset/tfrecords\"\n",
        "path_to_dataset = f\"./records_{model_name}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "W36WF4mT1kGK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training file names:  ['./records_lstm/train_2.tfrecords', './records_lstm/train_3.tfrecords', './records_lstm/train_1.tfrecords', './records_lstm/train_6.tfrecords', './records_lstm/train_0.tfrecords', './records_lstm/train_4.tfrecords', './records_lstm/train_5.tfrecords']\n",
            "Validation file names:  ['./records_lstm/val_0.tfrecords', './records_lstm/val_4.tfrecords', './records_lstm/val_2.tfrecords', './records_lstm/val_3.tfrecords', './records_lstm/val_1.tfrecords', './records_lstm/val_5.tfrecords']\n"
          ]
        }
      ],
      "source": [
        "# get training and validation tf record file names\n",
        "train_tfrecords_filenames = glob.glob(os.path.join(path_to_dataset, 'train_*'))\n",
        "val_tfrecords_filenames = glob.glob(os.path.join(path_to_dataset, 'val_*'))\n",
        "\n",
        "# suffle the file names for training\n",
        "np.random.shuffle(train_tfrecords_filenames)\n",
        "print(\"Training file names: \", train_tfrecords_filenames)\n",
        "print(\"Validation file names: \", val_tfrecords_filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "jKZtPoLMEmvJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "windowLength: 512\n",
            "overlap: 256\n",
            "ffTLength: 512\n",
            "inputFs: 48000.0\n",
            "fs: 16000.0\n",
            "numFeatures: 257\n",
            "numSegments: 63\n"
          ]
        }
      ],
      "source": [
        "# lstm\n",
        "windowLength = 512\n",
        "overlap      = round(0.5 * windowLength) # overlap of 75%\n",
        "ffTLength    = windowLength\n",
        "inputFs      = 48e3\n",
        "fs           = 16e3\n",
        "numFeatures  = ffTLength//2 + 1\n",
        "numSegments  = 63 # 1 sec in 512 window, 256 hop, sr = 16000 Hz\n",
        "\n",
        "print(\"windowLength:\",windowLength)\n",
        "print(\"overlap:\",overlap)\n",
        "print(\"ffTLength:\",ffTLength)\n",
        "print(\"inputFs:\",inputFs)\n",
        "print(\"fs:\",fs)\n",
        "print(\"numFeatures:\",numFeatures)\n",
        "print(\"numSegments:\",numSegments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzbdfIi-Lgk9"
      },
      "source": [
        "## Prepare Input features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "24kph3wJ3s5V"
      },
      "outputs": [],
      "source": [
        "def tf_record_parser(record):\n",
        "    keys_to_features = {\n",
        "        \"noise_stft_phase\": tf.io.FixedLenFeature((), tf.string, default_value=\"\"),\n",
        "        'noise_stft_mag_features': tf.io.FixedLenFeature([], tf.string),\n",
        "        \"clean_stft_magnitude\": tf.io.FixedLenFeature((), tf.string)\n",
        "    }\n",
        "\n",
        "    features = tf.io.parse_single_example(record, keys_to_features)\n",
        "\n",
        "    noise_stft_mag_features = tf.io.decode_raw(features['noise_stft_mag_features'], tf.float32)\n",
        "    clean_stft_magnitude = tf.io.decode_raw(features['clean_stft_magnitude'], tf.float32)\n",
        "    noise_stft_phase = tf.io.decode_raw(features['noise_stft_phase'], tf.float32)\n",
        "\n",
        "    # reshape input and annotation images, lstm\n",
        "    noise_stft_mag_features = tf.reshape(noise_stft_mag_features, (1, numSegments, numFeatures), name=\"noise_stft_mag_features\")\n",
        "    clean_stft_magnitude = tf.reshape(clean_stft_magnitude, (1, numSegments, numFeatures), name=\"clean_stft_magnitude\")\n",
        "    noise_stft_phase = tf.reshape(noise_stft_phase, (numFeatures,), name=\"noise_stft_phase\")\n",
        "\n",
        "    return noise_stft_mag_features, clean_stft_magnitude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJXPGrgVTCbZ"
      },
      "source": [
        "## Create tf.Data.Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "FF_A3YbZTCsj"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.TFRecordDataset([train_tfrecords_filenames])\n",
        "train_dataset = train_dataset.map(tf_record_parser)\n",
        "train_dataset = train_dataset.shuffle(8192)\n",
        "train_dataset = train_dataset.repeat()\n",
        "train_dataset = train_dataset.batch(512)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "NOuWPzQaTNDy"
      },
      "outputs": [],
      "source": [
        "test_dataset = tf.data.TFRecordDataset([val_tfrecords_filenames])\n",
        "test_dataset = test_dataset.map(tf_record_parser)\n",
        "test_dataset = test_dataset.repeat(1)\n",
        "test_dataset = test_dataset.batch(512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEG5JofLSfRw"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "2OZFegXrSYle"
      },
      "outputs": [],
      "source": [
        "from keras.layers import LSTM, Dense, BatchNormalization, Input, Layer, Multiply\n",
        "from keras import Model\n",
        "import keras.regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from librosa.filters import mel\n",
        "\n",
        "def get_mel_filter(samplerate, n_fft, n_mels, fmin, fmax):\n",
        "    # mel_basis = mel(sr=samplerate, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
        "    # return tf.Variable(init_value=mel_basis, dtype=tf.float32, trainable=False)\n",
        "    return tf.signal.linear_to_mel_weight_matrix(num_mel_bins=n_mels, num_spectrogram_bins=n_fft//2+1, sample_rate= samplerate,\n",
        "                                                lower_edge_hertz=fmin, upper_edge_hertz=fmax, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MelSpec(Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        frame_length=ffTLength,\n",
        "        frame_step=overlap,\n",
        "        fft_length=None,\n",
        "        sampling_rate=16000,\n",
        "        num_mel_channels=128,\n",
        "        freq_min=125,\n",
        "        freq_max=8000,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.frame_length = frame_length\n",
        "        self.frame_step = frame_step\n",
        "        self.fft_length = fft_length\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.num_mel_channels = num_mel_channels\n",
        "        self.freq_min = freq_min\n",
        "        self.freq_max = freq_max\n",
        "        \n",
        "        # Defining mel filter. This filter will be multiplied with the STFT output\n",
        "        self.mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n",
        "            num_mel_bins=self.num_mel_channels,\n",
        "            num_spectrogram_bins=self.frame_length // 2 + 1,\n",
        "            sample_rate=self.sampling_rate,\n",
        "            lower_edge_hertz=self.freq_min,\n",
        "            upper_edge_hertz=self.freq_max,\n",
        "        )\n",
        "\n",
        "    def call(self, magnitude, training=True):\n",
        "        # We will only perform the transformation during training.\n",
        "        mel = tf.matmul(tf.square(magnitude), self.mel_filterbank)\n",
        "        return mel\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(MelSpec, self).get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"frame_length\": self.frame_length,\n",
        "                \"frame_step\": self.frame_step,\n",
        "                \"fft_length\": self.fft_length,\n",
        "                \"sampling_rate\": self.sampling_rate,\n",
        "                \"num_mel_channels\": self.num_mel_channels,\n",
        "                \"freq_min\": self.freq_min,\n",
        "                \"freq_max\": self.freq_max,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "class InverseMelSpec(Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        frame_length=ffTLength,\n",
        "        frame_step=overlap,\n",
        "        fft_length=None,\n",
        "        sampling_rate=16000,\n",
        "        num_mel_channels=128,\n",
        "        freq_min=125,\n",
        "        freq_max=8000,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.frame_length = frame_length\n",
        "        self.frame_step = frame_step\n",
        "        self.fft_length = fft_length\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.num_mel_channels = num_mel_channels\n",
        "        self.freq_min = freq_min\n",
        "        self.freq_max = freq_max\n",
        "        \n",
        "        # Defining mel filter. This filter will be multiplied with the STFT output\n",
        "        self.mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n",
        "            num_mel_bins=self.num_mel_channels,\n",
        "            num_spectrogram_bins=self.frame_length // 2 + 1,\n",
        "            sample_rate=self.sampling_rate,\n",
        "            lower_edge_hertz=self.freq_min,\n",
        "            upper_edge_hertz=self.freq_max,\n",
        "        )\n",
        "\n",
        "    def call(self, magnitude, training=True):\n",
        "        # We will only perform the transformation during training.\n",
        "        mel = tf.matmul(magnitude, tf.transpose(self.mel_filterbank, perm=[1, 0]))\n",
        "        return mel\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(InverseMelSpec, self).get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"frame_length\": self.frame_length,\n",
        "                \"frame_step\": self.frame_step,\n",
        "                \"fft_length\": self.fft_length,\n",
        "                \"sampling_rate\": self.sampling_rate,\n",
        "                \"num_mel_channels\": self.num_mel_channels,\n",
        "                \"freq_min\": self.freq_min,\n",
        "                \"freq_max\": self.freq_max,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model_rnn():\n",
        "  inputs = Input(shape=[1, numSegments, numFeatures])\n",
        "  \n",
        "  x = inputs\n",
        "\n",
        "  mask = tf.squeeze(x, axis=1) # merge channel\n",
        "  mask = MelSpec()(mask)\n",
        "  mask = LSTM(256, activation='tanh', return_sequences=True)(mask)\n",
        "  mask = Dense(128, activation='relu', use_bias=True, \n",
        "        kernel_initializer='glorot_uniform', bias_initializer='zeros')(mask)\n",
        "\n",
        "  mask = BatchNormalization()(mask)\n",
        "\n",
        "  mask = LSTM(256, activation='tanh', return_sequences=True)(mask)\n",
        "  mask = Dense(128, activation='sigmoid', use_bias=True,\n",
        "        kernel_initializer='glorot_uniform', bias_initializer='zeros')(mask)\n",
        "  \n",
        "  mask = InverseMelSpec()(mask)\n",
        "  mask = tf.expand_dims(mask, axis=1) # merge channel\n",
        "  \n",
        "  x = Multiply()([x, mask])\n",
        "  model = Model(inputs=inputs, outputs=x)\n",
        "\n",
        "  optimizer = keras.optimizers.Adam(3e-4)\n",
        "  #optimizer = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=3e-4)\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss='mse', \n",
        "              metrics=[keras.metrics.RootMeanSquaredError('rmse')])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "mNpHS4LuShxd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 1, 63, 257)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_3 (TFOpLa  (None, 63, 257)     0           ['input_4[0][0]']                \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " mel_spec_3 (MelSpec)           (None, 63, 128)      0           ['tf.compat.v1.squeeze_3[0][0]'] \n",
            "                                                                                                  \n",
            " lstm_6 (LSTM)                  (None, 63, 256)      394240      ['mel_spec_3[0][0]']             \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 63, 128)      32896       ['lstm_6[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 63, 128)     512         ['dense_6[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " lstm_7 (LSTM)                  (None, 63, 256)      394240      ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 63, 128)      32896       ['lstm_7[0][0]']                 \n",
            "                                                                                                  \n",
            " inverse_mel_spec_3 (InverseMel  (None, 63, 257)     0           ['dense_7[0][0]']                \n",
            " Spec)                                                                                            \n",
            "                                                                                                  \n",
            " tf.expand_dims_3 (TFOpLambda)  (None, 1, 63, 257)   0           ['inverse_mel_spec_3[0][0]']     \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 1, 63, 257)   0           ['input_4[0][0]',                \n",
            "                                                                  'tf.expand_dims_3[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 854,784\n",
            "Trainable params: 854,528\n",
            "Non-trainable params: 256\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_model_rnn()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "8rWOgsdwglFE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
          ]
        }
      ],
      "source": [
        "# You might need to install the following dependencies: sudo apt install python-pydot python-pydot-ng graphviz\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, dpi=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "7OdmgHTp4_Ou"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6007 (pid 498806), started 1 day, 3:19:28 ago. (Use '!kill 498806' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-949aeae73e4d3bf9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-949aeae73e4d3bf9\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6007;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "BucSAwkHQj8Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 1s 21ms/step - loss: 0.3766 - rmse: 0.6136\n",
            "Baseline accuracy 0.37655094265937805\n"
          ]
        }
      ],
      "source": [
        "# [TODO] GPU memory overflow, try to python\n",
        "baseline_val_loss = model.evaluate(test_dataset)[0]\n",
        "print(f\"Baseline accuracy {baseline_val_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "ocycFbP5-X0o"
      },
      "outputs": [],
      "source": [
        "def l2_norm(vector):\n",
        "    return np.square(vector)\n",
        "\n",
        "def SDR(denoised, cleaned, eps=1e-7): # Signal to Distortion Ratio\n",
        "    a = l2_norm(denoised)\n",
        "    b = l2_norm(denoised - cleaned)\n",
        "    a_b = a / b\n",
        "    return np.mean(10 * np.log10(a_b + eps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "bcPAuMZ9SlHa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "600/600 [==============================] - 21s 33ms/step - loss: 0.2292 - rmse: 0.4787 - val_loss: 0.2123 - val_rmse: 0.4607\n",
            "Epoch 2/400\n",
            "600/600 [==============================] - 20s 33ms/step - loss: 0.2023 - rmse: 0.4498 - val_loss: 0.2085 - val_rmse: 0.4566\n",
            "Epoch 3/400\n",
            "600/600 [==============================] - 19s 32ms/step - loss: 0.1968 - rmse: 0.4436 - val_loss: 0.2078 - val_rmse: 0.4558\n",
            "Epoch 4/400\n",
            "600/600 [==============================] - 19s 31ms/step - loss: 0.1934 - rmse: 0.4397 - val_loss: 0.2078 - val_rmse: 0.4559\n",
            "Epoch 5/400\n",
            "600/600 [==============================] - 19s 31ms/step - loss: 0.1912 - rmse: 0.4372 - val_loss: 0.2071 - val_rmse: 0.4550\n",
            "Epoch 6/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1893 - rmse: 0.4351 - val_loss: 0.2068 - val_rmse: 0.4548\n",
            "Epoch 7/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1880 - rmse: 0.4336 - val_loss: 0.2067 - val_rmse: 0.4546\n",
            "Epoch 8/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1871 - rmse: 0.4325 - val_loss: 0.2066 - val_rmse: 0.4545\n",
            "Epoch 9/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1860 - rmse: 0.4313 - val_loss: 0.2068 - val_rmse: 0.4547\n",
            "Epoch 10/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1852 - rmse: 0.4304 - val_loss: 0.2070 - val_rmse: 0.4550\n",
            "Epoch 11/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1852 - rmse: 0.4304 - val_loss: 0.2068 - val_rmse: 0.4548\n",
            "Epoch 12/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1839 - rmse: 0.4289 - val_loss: 0.2078 - val_rmse: 0.4558\n",
            "Epoch 13/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1835 - rmse: 0.4283 - val_loss: 0.2080 - val_rmse: 0.4561\n",
            "Epoch 14/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1833 - rmse: 0.4282 - val_loss: 0.2077 - val_rmse: 0.4557\n",
            "Epoch 15/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1827 - rmse: 0.4274 - val_loss: 0.2079 - val_rmse: 0.4560\n",
            "Epoch 16/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1824 - rmse: 0.4271 - val_loss: 0.2081 - val_rmse: 0.4562\n",
            "Epoch 17/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1819 - rmse: 0.4265 - val_loss: 0.2089 - val_rmse: 0.4571\n",
            "Epoch 18/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1817 - rmse: 0.4262 - val_loss: 0.2092 - val_rmse: 0.4574\n",
            "Epoch 19/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1814 - rmse: 0.4260 - val_loss: 0.2090 - val_rmse: 0.4571\n",
            "Epoch 20/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1812 - rmse: 0.4257 - val_loss: 0.2090 - val_rmse: 0.4572\n",
            "Epoch 21/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1809 - rmse: 0.4254 - val_loss: 0.2094 - val_rmse: 0.4576\n",
            "Epoch 22/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1806 - rmse: 0.4250 - val_loss: 0.2092 - val_rmse: 0.4574\n",
            "Epoch 23/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1805 - rmse: 0.4249 - val_loss: 0.2095 - val_rmse: 0.4578\n",
            "Epoch 24/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1802 - rmse: 0.4245 - val_loss: 0.2097 - val_rmse: 0.4579\n",
            "Epoch 25/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1801 - rmse: 0.4244 - val_loss: 0.2102 - val_rmse: 0.4585\n",
            "Epoch 26/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1810 - rmse: 0.4255 - val_loss: 0.2089 - val_rmse: 0.4570\n",
            "Epoch 27/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1796 - rmse: 0.4238 - val_loss: 0.2100 - val_rmse: 0.4582\n",
            "Epoch 28/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1795 - rmse: 0.4237 - val_loss: 0.2104 - val_rmse: 0.4587\n",
            "Epoch 29/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1795 - rmse: 0.4236 - val_loss: 0.2105 - val_rmse: 0.4588\n",
            "Epoch 30/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1794 - rmse: 0.4235 - val_loss: 0.2103 - val_rmse: 0.4585\n",
            "Epoch 31/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1792 - rmse: 0.4233 - val_loss: 0.2108 - val_rmse: 0.4591\n",
            "Epoch 32/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1791 - rmse: 0.4232 - val_loss: 0.2108 - val_rmse: 0.4592\n",
            "Epoch 33/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1800 - rmse: 0.4243 - val_loss: 0.2091 - val_rmse: 0.4573\n",
            "Epoch 34/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1788 - rmse: 0.4228 - val_loss: 0.2109 - val_rmse: 0.4593\n",
            "Epoch 35/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1787 - rmse: 0.4227 - val_loss: 0.2110 - val_rmse: 0.4594\n",
            "Epoch 36/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1787 - rmse: 0.4228 - val_loss: 0.2109 - val_rmse: 0.4592\n",
            "Epoch 37/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1784 - rmse: 0.4224 - val_loss: 0.2116 - val_rmse: 0.4600\n",
            "Epoch 38/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1786 - rmse: 0.4226 - val_loss: 0.2112 - val_rmse: 0.4596\n",
            "Epoch 39/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1784 - rmse: 0.4223 - val_loss: 0.2116 - val_rmse: 0.4600\n",
            "Epoch 40/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1783 - rmse: 0.4222 - val_loss: 0.2114 - val_rmse: 0.4598\n",
            "Epoch 41/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1784 - rmse: 0.4223 - val_loss: 0.2115 - val_rmse: 0.4599\n",
            "Epoch 42/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1781 - rmse: 0.4220 - val_loss: 0.2121 - val_rmse: 0.4606\n",
            "Epoch 43/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1784 - rmse: 0.4223 - val_loss: 0.2117 - val_rmse: 0.4601\n",
            "Epoch 44/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1778 - rmse: 0.4217 - val_loss: 0.2117 - val_rmse: 0.4601\n",
            "Epoch 45/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1780 - rmse: 0.4219 - val_loss: 0.2122 - val_rmse: 0.4606\n",
            "Epoch 46/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1778 - rmse: 0.4217 - val_loss: 0.2118 - val_rmse: 0.4602\n",
            "Epoch 47/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1778 - rmse: 0.4217 - val_loss: 0.2127 - val_rmse: 0.4612\n",
            "Epoch 48/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1777 - rmse: 0.4216 - val_loss: 0.2124 - val_rmse: 0.4608\n",
            "Epoch 49/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1777 - rmse: 0.4215 - val_loss: 0.2127 - val_rmse: 0.4612\n",
            "Epoch 50/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1776 - rmse: 0.4214 - val_loss: 0.2125 - val_rmse: 0.4610\n",
            "Epoch 51/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1776 - rmse: 0.4214 - val_loss: 0.2123 - val_rmse: 0.4607\n",
            "Epoch 52/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1774 - rmse: 0.4212 - val_loss: 0.2124 - val_rmse: 0.4609\n",
            "Epoch 53/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1774 - rmse: 0.4212 - val_loss: 0.2129 - val_rmse: 0.4614\n",
            "Epoch 54/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1787 - rmse: 0.4228 - val_loss: 0.2116 - val_rmse: 0.4600\n",
            "Epoch 55/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1772 - rmse: 0.4210 - val_loss: 0.2128 - val_rmse: 0.4613\n",
            "Epoch 56/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1784 - rmse: 0.4223 - val_loss: 0.2123 - val_rmse: 0.4607\n",
            "Epoch 57/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1771 - rmse: 0.4209 - val_loss: 0.2126 - val_rmse: 0.4611\n",
            "Epoch 58/400\n",
            "600/600 [==============================] - 18s 30ms/step - loss: 0.1771 - rmse: 0.4208 - val_loss: 0.2125 - val_rmse: 0.4609\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5c2262d460>"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, baseline=None)\n",
        "logdir = os.path.join(f\"logs/{model_name}\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch')\n",
        "\n",
        "save_path = f'./result/{model_name}/checkpoint' + \"/model-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, \n",
        "                                                         monitor='val_loss', save_best_only=True)\n",
        "\n",
        "model.fit(train_dataset,\n",
        "         steps_per_epoch=600, # you might need to change this\n",
        "         validation_data=test_dataset,\n",
        "         epochs=400,\n",
        "         callbacks=[early_stopping_callback, tensorboard_callback, checkpoint_callback]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "g1ZZskDZ5L2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 1s 23ms/step - loss: 0.2066 - rmse: 0.4545\n",
            "New model saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: .result/lstm/model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: .result/lstm/model/assets\n"
          ]
        }
      ],
      "source": [
        "import keras.models\n",
        "val_loss = model.evaluate(test_dataset)[0]\n",
        "if val_loss < baseline_val_loss:\n",
        "  print(\"New model saved.\")\n",
        "  keras.models.save_model(model, f'./result/{model_name}/model', overwrite=True, include_optimizer=True)\n",
        "  # model.save('./denoiser_cnn_log_mel_generator.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeJTsGxCSuhm"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLiTiK8blE0n"
      },
      "outputs": [],
      "source": [
        "def read_audio(filepath, sample_rate, normalize=True):\n",
        "    \"\"\"Read an audio file and return it as a numpy array\"\"\"\n",
        "    audio, sr = librosa.load(filepath, sr=sample_rate)\n",
        "    if normalize:\n",
        "      div_fac = 1 / np.max(np.abs(audio)) / 3.0\n",
        "      audio = audio * div_fac\n",
        "    return audio, sr\n",
        "        \n",
        "def add_noise_to_clean_audio(clean_audio, noise_signal):\n",
        "    \"\"\"Adds noise to an audio sample\"\"\"\n",
        "    if len(clean_audio) >= len(noise_signal):\n",
        "        # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n",
        "        while len(clean_audio) >= len(noise_signal):\n",
        "            noise_signal = np.append(noise_signal, noise_signal)\n",
        "\n",
        "    ## Extract a noise segment from a random location in the noise file\n",
        "    ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n",
        "\n",
        "    noiseSegment = noise_signal[ind: ind + clean_audio.size]\n",
        "\n",
        "    speech_power = np.sum(clean_audio ** 2)\n",
        "    noise_power = np.sum(noiseSegment ** 2)\n",
        "    noisyAudio = clean_audio + np.sqrt(speech_power / noise_power) * noiseSegment\n",
        "    return noisyAudio\n",
        "\n",
        "def play(audio, sample_rate):\n",
        "    ipd.display(ipd.Audio(data=audio, rate=sample_rate))  # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM6ajbBFlx3b"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor:\n",
        "    def __init__(self, audio, *, windowLength, overlap, sample_rate):\n",
        "        self.audio = audio\n",
        "        self.ffT_length = windowLength\n",
        "        self.window_length = windowLength\n",
        "        self.overlap = overlap\n",
        "        self.sample_rate = sample_rate\n",
        "        self.window = scipy.signal.hamming(self.window_length, sym=False)\n",
        "\n",
        "    def get_stft_spectrogram(self):\n",
        "        return librosa.stft(self.audio, n_fft=self.ffT_length, win_length=self.window_length, hop_length=self.overlap,\n",
        "                            window=self.window, center=True)\n",
        "\n",
        "    def get_audio_from_stft_spectrogram(self, stft_features):\n",
        "        return librosa.istft(stft_features, win_length=self.window_length, hop_length=self.overlap,\n",
        "                             window=self.window, center=True)\n",
        "\n",
        "    def get_mel_spectrogram(self):\n",
        "        return librosa.feature.melspectrogram(self.audio, sr=self.sample_rate, power=2.0, pad_mode='reflect',\n",
        "                                           n_fft=self.ffT_length, hop_length=self.overlap, center=True)\n",
        "\n",
        "    def get_audio_from_mel_spectrogram(self, M):\n",
        "        return librosa.feature.inverse.mel_to_audio(M, sr=self.sample_rate, n_fft=self.ffT_length, hop_length=self.overlap,\n",
        "                                             win_length=self.window_length, window=self.window,\n",
        "                                             center=True, pad_mode='reflect', power=2.0, n_iter=32, length=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dcnyvquSoLs"
      },
      "outputs": [],
      "source": [
        "cleanAudio, sr = read_audio(os.path.join(mozilla_basepath, 'test', 'common_voice_en_16526.mp3'), sample_rate=fs)\n",
        "print(\"Min:\", np.min(cleanAudio),\"Max:\",np.max(cleanAudio))\n",
        "ipd.Audio(data=cleanAudio, rate=sr) # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaHe1okPTvV-"
      },
      "outputs": [],
      "source": [
        "noiseAudio, sr = read_audio(os.path.join(UrbanSound8K_basepath, 'test', '7913-3-0-0.wav'), sample_rate=fs)\n",
        "print(\"Min:\", np.min(noiseAudio),\"Max:\",np.max(noiseAudio))\n",
        "ipd.Audio(data=noiseAudio, rate=sr) # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu_eOKRfTHbp"
      },
      "outputs": [],
      "source": [
        "cleanAudioFeatureExtractor = FeatureExtractor(cleanAudio, windowLength=windowLength, overlap=overlap, sample_rate=sr)\n",
        "stft_features = cleanAudioFeatureExtractor.get_stft_spectrogram()\n",
        "stft_features = np.abs(stft_features)\n",
        "print(\"Min:\", np.min(stft_features),\"Max:\",np.max(stft_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHWcmobyTP4E"
      },
      "outputs": [],
      "source": [
        "noisyAudio = add_noise_to_clean_audio(cleanAudio, noiseAudio)\n",
        "ipd.Audio(data=noisyAudio, rate=fs) # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75M29dl3bBeF"
      },
      "outputs": [],
      "source": [
        "def prepare_input_features(stft_features):\n",
        "    # Phase Aware Scaling: To avoid extreme differences (more than\n",
        "    # 45 degree) between the noisy and clean phase, the clean spectral magnitude was encoded as similar to [21]:\n",
        "    noisySTFT = np.concatenate([stft_features[:,0:numSegments-1], stft_features], axis=1)\n",
        "    stftSegments = np.zeros((numFeatures, numSegments , noisySTFT.shape[1] - numSegments + 1))\n",
        "\n",
        "    for index in range(noisySTFT.shape[1] - numSegments + 1):\n",
        "        stftSegments[:,:,index] = noisySTFT[:,index:index + numSegments]\n",
        "    return stftSegments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cjO5-cjTP6t"
      },
      "outputs": [],
      "source": [
        "noiseAudioFeatureExtractor = FeatureExtractor(noisyAudio, windowLength=windowLength, overlap=overlap, sample_rate=sr)\n",
        "noise_stft_features = noiseAudioFeatureExtractor.get_stft_spectrogram()\n",
        "\n",
        "# Paper: Besides, spectral phase was not used in the training phase.\n",
        "# At reconstruction, noisy spectral phase was used instead to\n",
        "# perform in- verse STFT and recover human speech.\n",
        "noisyPhase = np.angle(noise_stft_features)\n",
        "print(noisyPhase.shape)\n",
        "noise_stft_features = np.abs(noise_stft_features)\n",
        "\n",
        "mean = np.mean(noise_stft_features)\n",
        "std = np.std(noise_stft_features)\n",
        "noise_stft_features = (noise_stft_features - mean) / std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3p5PWWkrlE3m"
      },
      "outputs": [],
      "source": [
        "predictors = prepare_input_features(noise_stft_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pSoOw2fTP9N"
      },
      "outputs": [],
      "source": [
        "predictors = np.reshape(predictors, (predictors.shape[0], predictors.shape[1], 1, predictors.shape[2]))\n",
        "predictors = np.transpose(predictors, (3, 0, 1, 2)).astype(np.float32)\n",
        "print('predictors.shape:', predictors.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5sNR4sSTP_1"
      },
      "outputs": [],
      "source": [
        "STFTFullyConvolutional = model.predict(predictors)\n",
        "print(STFTFullyConvolutional.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzCga3PdUVwG"
      },
      "outputs": [],
      "source": [
        "def revert_features_to_audio(features, phase, cleanMean=None, cleanStd=None):\n",
        "    # scale the outpus back to the original range\n",
        "    if cleanMean and cleanStd:\n",
        "        features = cleanStd * features + cleanMean\n",
        "\n",
        "    phase = np.transpose(phase, (1, 0))\n",
        "    features = np.squeeze(features)\n",
        "\n",
        "    # features = librosa.db_to_power(features)\n",
        "    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n",
        "\n",
        "    features = np.transpose(features, (1, 0))\n",
        "    return noiseAudioFeatureExtractor.get_audio_from_stft_spectrogram(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWlUDtPzURlQ"
      },
      "outputs": [],
      "source": [
        "denoisedAudioFullyConvolutional = revert_features_to_audio(STFTFullyConvolutional, noisyPhase, mean, std)\n",
        "print(\"Min:\", np.min(denoisedAudioFullyConvolutional),\"Max:\",np.max(denoisedAudioFullyConvolutional))\n",
        "ipd.Audio(data=denoisedAudioFullyConvolutional, rate=fs) # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvEIBy7EHgeP"
      },
      "outputs": [],
      "source": [
        "# A numeric identifier of the sound class -- Types of noise\n",
        "# 0 = air_conditioner\n",
        "# 1 = car_horn\n",
        "# 2 = children_playing\n",
        "# 3 = dog_bark\n",
        "# 4 = drilling\n",
        "# 5 = engine_idling\n",
        "# 6 = gun_shot\n",
        "# 7 = jackhammer\n",
        "# 8 = siren\n",
        "# 9 = street_music"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv_7ZwWaUW0_"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2, ax3) = plt.subplots(3, 1, sharey=True)\n",
        "\n",
        "ax1.plot(cleanAudio)\n",
        "ax1.set_title(\"Clean Audio\")\n",
        "\n",
        "ax2.plot(noisyAudio)\n",
        "ax2.set_title(\"Noisy Audio\")\n",
        "\n",
        "ax3.plot(denoisedAudioFullyConvolutional)\n",
        "ax3.set_title(\"Denoised Audio\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LM7E91avKA3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "SpeechDenoiserCNN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 ('tf_29_daniel')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "904f93ba5a280e572257a04f19a20f81b654f882c045508efe9071ed46139c46"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
