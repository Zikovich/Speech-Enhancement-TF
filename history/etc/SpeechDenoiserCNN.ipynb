{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "VoyZ5yLRaAeI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-09-05 16:50:55.499084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-05 16:50:55.499370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-05 16:50:55.499481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-05 16:50:55.499879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-05 16:50:55.500013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-05 16:50:55.500120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 21707 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "Ah7ISzvtEmur"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import IPython.display as ipd\n",
        "import librosa.display\n",
        "import scipy\n",
        "import glob\n",
        "import numpy as np\n",
        "import math\n",
        "import warnings\n",
        "import pickle\n",
        "from sklearn.utils import shuffle\n",
        "import zipfile\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "WBZyUMpobR-Z"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-09-05 16:50:55.536665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 4944271477128140299\n",
              " xla_global_id: -1,\n",
              " name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 22761504768\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 1189508342212309880\n",
              " physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
              " xla_global_id: 416903419]"
            ]
          },
          "execution_count": 180,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-09-05 16:50:55.536901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-05 16:50:55.537010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-05 16:50:55.537151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-05 16:50:55.537259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-09-05 16:50:55.537365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 21707 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "zu2lgR8lEmu0"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(999)\n",
        "np.random.seed(999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "JGbkvQHxxrj_"
      },
      "outputs": [],
      "source": [
        "# !wget 'cdn.daitan.com/dataset.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "51o6Ze9hxrkE"
      },
      "outputs": [],
      "source": [
        "# dataset_file_name = './dataset.zip'\n",
        "# with zipfile.ZipFile(dataset_file_name, 'r') as zip_ref:\n",
        "#     zip_ref.extractall('./dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"cnn\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "d43UvhEqxrkJ"
      },
      "outputs": [],
      "source": [
        "# path_to_dataset = \"./dataset/tfrecords\"\n",
        "path_to_dataset = f\"./records_{model_name}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "W36WF4mT1kGK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training file names:  ['./records_lstm/train_3.tfrecords', './records_lstm/train_4.tfrecords', './records_lstm/train_1.tfrecords', './records_lstm/train_6.tfrecords', './records_lstm/train_5.tfrecords', './records_lstm/train_2.tfrecords', './records_lstm/train_0.tfrecords']\n",
            "Validation file names:  ['./records_lstm/val_0.tfrecords', './records_lstm/val_4.tfrecords', './records_lstm/val_2.tfrecords', './records_lstm/val_3.tfrecords', './records_lstm/val_1.tfrecords', './records_lstm/val_5.tfrecords']\n"
          ]
        }
      ],
      "source": [
        "# get training and validation tf record file names\n",
        "train_tfrecords_filenames = glob.glob(os.path.join(path_to_dataset, 'train_*'))\n",
        "val_tfrecords_filenames = glob.glob(os.path.join(path_to_dataset, 'val_*'))\n",
        "\n",
        "# suffle the file names for training\n",
        "np.random.shuffle(train_tfrecords_filenames)\n",
        "print(\"Training file names: \", train_tfrecords_filenames)\n",
        "print(\"Validation file names: \", val_tfrecords_filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "jKZtPoLMEmvJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "windowLength: 256\n",
            "overlap: 64\n",
            "ffTLength: 256\n",
            "inputFs: 48000.0\n",
            "fs: 16000.0\n",
            "numFeatures: 129\n",
            "numSegments: 8\n"
          ]
        }
      ],
      "source": [
        "windowLength = 256\n",
        "overlap      = round(0.25 * windowLength) # overlap of 75%\n",
        "ffTLength    = windowLength\n",
        "inputFs      = 48e3\n",
        "fs           = 16e3\n",
        "numFeatures  = ffTLength//2 + 1\n",
        "numSegments  = 8\n",
        "print(\"windowLength:\",windowLength)\n",
        "print(\"overlap:\",overlap)\n",
        "print(\"ffTLength:\",ffTLength)\n",
        "print(\"inputFs:\",inputFs)\n",
        "print(\"fs:\",fs)\n",
        "print(\"numFeatures:\",numFeatures)\n",
        "print(\"numSegments:\",numSegments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "JXpSGe8L0cl_"
      },
      "outputs": [],
      "source": [
        "mozilla_basepath = \"./dataset/en\"\n",
        "UrbanSound8K_basepath = './dataset/UrbanSound8K'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzbdfIi-Lgk9"
      },
      "source": [
        "## Prepare Input features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "24kph3wJ3s5V"
      },
      "outputs": [],
      "source": [
        "def tf_record_parser(record):\n",
        "    keys_to_features = {\n",
        "        \"noise_stft_phase\": tf.io.FixedLenFeature((), tf.string, default_value=\"\"),\n",
        "        'noise_stft_mag_features': tf.io.FixedLenFeature([], tf.string),\n",
        "        \"clean_stft_magnitude\": tf.io.FixedLenFeature((), tf.string)\n",
        "    }\n",
        "\n",
        "    features = tf.io.parse_single_example(record, keys_to_features)\n",
        "\n",
        "    noise_stft_mag_features = tf.io.decode_raw(features['noise_stft_mag_features'], tf.float32)\n",
        "    clean_stft_magnitude = tf.io.decode_raw(features['clean_stft_magnitude'], tf.float32)\n",
        "    noise_stft_phase = tf.io.decode_raw(features['noise_stft_phase'], tf.float32)\n",
        "\n",
        "    # reshape input and annotation images\n",
        "    noise_stft_mag_features = tf.reshape(noise_stft_mag_features, (129, 8, 1), name=\"noise_stft_mag_features\")\n",
        "    clean_stft_magnitude = tf.reshape(clean_stft_magnitude, (129, 1, 1), name=\"clean_stft_magnitude\")\n",
        "    noise_stft_phase = tf.reshape(noise_stft_phase, (129,), name=\"noise_stft_phase\")\n",
        "\n",
        "    return noise_stft_mag_features, clean_stft_magnitude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJXPGrgVTCbZ"
      },
      "source": [
        "## Create tf.Data.Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "FF_A3YbZTCsj"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.TFRecordDataset([train_tfrecords_filenames])\n",
        "train_dataset = train_dataset.map(tf_record_parser)\n",
        "train_dataset = train_dataset.shuffle(8192)\n",
        "train_dataset = train_dataset.repeat()\n",
        "train_dataset = train_dataset.batch(512)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "NOuWPzQaTNDy"
      },
      "outputs": [],
      "source": [
        "test_dataset = tf.data.TFRecordDataset([val_tfrecords_filenames])\n",
        "test_dataset = test_dataset.map(tf_record_parser)\n",
        "test_dataset = test_dataset.repeat(1)\n",
        "test_dataset = test_dataset.batch(512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None, 129, 8, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 129, 1, 1), dtype=tf.float32, name=None))>"
            ]
          },
          "execution_count": 212,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEG5JofLSfRw"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "2OZFegXrSYle"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv2D, Input, LeakyReLU, Flatten, Dense, Reshape, Conv2DTranspose, BatchNormalization, Activation, ZeroPadding2D, SpatialDropout2D\n",
        "from keras import Model, Sequential\n",
        "# import keras.layers\n",
        "import keras.regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "lc2K0cfhPU5-"
      },
      "outputs": [],
      "source": [
        "def conv_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n",
        "  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, use_bias=False,\n",
        "              kernel_regularizer=tf.keras.regularizers.l2(0.0006))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  if use_bn:\n",
        "    x = BatchNormalization()(x)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "XFYyKoAVQYCz"
      },
      "outputs": [],
      "source": [
        "def full_pre_activation_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n",
        "  shortcut = x\n",
        "  in_channels = x.shape[-1]\n",
        "\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(filters=in_channels, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "\n",
        "  return shortcut + x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "NlRQ3ngpZG1Y"
      },
      "outputs": [],
      "source": [
        "def build_model(l2_strength):\n",
        "  inputs = Input(shape=[numFeatures, numSegments, 1])\n",
        "  x = inputs\n",
        "  \n",
        "  # -----\n",
        "  x = ZeroPadding2D(((4,4), (0,0)))(x)\n",
        "  x = Conv2D(filters=18, kernel_size=[9,8], strides=[1, 1], padding='valid', use_bias=False,\n",
        "              kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  skip0 = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "                 kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = Activation('relu')(skip0)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # -----\n",
        "  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  skip1 = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "                 kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = Activation('relu')(skip1)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # ----\n",
        "  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  \n",
        "  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # ----\n",
        "  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "             kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = x + skip1\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # ----\n",
        "  x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "             kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = x + skip0\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
        "              kernel_regularizer=keras.regularizers.l2(l2_strength))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # ----\n",
        "  x = SpatialDropout2D(0.2)(x)\n",
        "  x = Conv2D(filters=1, kernel_size=[129,1], strides=[1, 1], padding='same')(x)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=x)\n",
        "\n",
        "  optimizer = keras.optimizers.Adam(3e-4)\n",
        "  #optimizer = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=3e-4)\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss='mse', \n",
        "                metrics=[keras.metrics.RootMeanSquaredError('rmse')])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "mNpHS4LuShxd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 129, 8, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " zero_padding2d_10 (ZeroPadding  (None, 137, 8, 1)   0           ['input_11[0][0]']               \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " conv2d_145 (Conv2D)            (None, 129, 1, 18)   1296        ['zero_padding2d_10[0][0]']      \n",
            "                                                                                                  \n",
            " activation_135 (Activation)    (None, 129, 1, 18)   0           ['conv2d_145[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_135 (Batch  (None, 129, 1, 18)  72          ['activation_135[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_146 (Conv2D)            (None, 129, 1, 30)   2700        ['batch_normalization_135[0][0]']\n",
            "                                                                                                  \n",
            " activation_136 (Activation)    (None, 129, 1, 30)   0           ['conv2d_146[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_136 (Batch  (None, 129, 1, 30)  120         ['activation_136[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_147 (Conv2D)            (None, 129, 1, 8)    2160        ['batch_normalization_136[0][0]']\n",
            "                                                                                                  \n",
            " activation_137 (Activation)    (None, 129, 1, 8)    0           ['conv2d_147[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_137 (Batch  (None, 129, 1, 8)   32          ['activation_137[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_148 (Conv2D)            (None, 129, 1, 18)   1296        ['batch_normalization_137[0][0]']\n",
            "                                                                                                  \n",
            " activation_138 (Activation)    (None, 129, 1, 18)   0           ['conv2d_148[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_138 (Batch  (None, 129, 1, 18)  72          ['activation_138[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_149 (Conv2D)            (None, 129, 1, 30)   2700        ['batch_normalization_138[0][0]']\n",
            "                                                                                                  \n",
            " activation_139 (Activation)    (None, 129, 1, 30)   0           ['conv2d_149[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_139 (Batch  (None, 129, 1, 30)  120         ['activation_139[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_150 (Conv2D)            (None, 129, 1, 8)    2160        ['batch_normalization_139[0][0]']\n",
            "                                                                                                  \n",
            " activation_140 (Activation)    (None, 129, 1, 8)    0           ['conv2d_150[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_140 (Batch  (None, 129, 1, 8)   32          ['activation_140[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_151 (Conv2D)            (None, 129, 1, 18)   1296        ['batch_normalization_140[0][0]']\n",
            "                                                                                                  \n",
            " activation_141 (Activation)    (None, 129, 1, 18)   0           ['conv2d_151[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_141 (Batch  (None, 129, 1, 18)  72          ['activation_141[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_152 (Conv2D)            (None, 129, 1, 30)   2700        ['batch_normalization_141[0][0]']\n",
            "                                                                                                  \n",
            " activation_142 (Activation)    (None, 129, 1, 30)   0           ['conv2d_152[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_142 (Batch  (None, 129, 1, 30)  120         ['activation_142[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_153 (Conv2D)            (None, 129, 1, 8)    2160        ['batch_normalization_142[0][0]']\n",
            "                                                                                                  \n",
            " activation_143 (Activation)    (None, 129, 1, 8)    0           ['conv2d_153[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_143 (Batch  (None, 129, 1, 8)   32          ['activation_143[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_154 (Conv2D)            (None, 129, 1, 18)   1296        ['batch_normalization_143[0][0]']\n",
            "                                                                                                  \n",
            " activation_144 (Activation)    (None, 129, 1, 18)   0           ['conv2d_154[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_144 (Batch  (None, 129, 1, 18)  72          ['activation_144[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_155 (Conv2D)            (None, 129, 1, 30)   2700        ['batch_normalization_144[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_18 (TFOpL  (None, 129, 1, 30)  0           ['conv2d_155[0][0]',             \n",
            " ambda)                                                           'conv2d_149[0][0]']             \n",
            "                                                                                                  \n",
            " activation_145 (Activation)    (None, 129, 1, 30)   0           ['tf.__operators__.add_18[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_145 (Batch  (None, 129, 1, 30)  120         ['activation_145[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_156 (Conv2D)            (None, 129, 1, 8)    2160        ['batch_normalization_145[0][0]']\n",
            "                                                                                                  \n",
            " activation_146 (Activation)    (None, 129, 1, 8)    0           ['conv2d_156[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_146 (Batch  (None, 129, 1, 8)   32          ['activation_146[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_157 (Conv2D)            (None, 129, 1, 18)   1296        ['batch_normalization_146[0][0]']\n",
            "                                                                                                  \n",
            " activation_147 (Activation)    (None, 129, 1, 18)   0           ['conv2d_157[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_147 (Batch  (None, 129, 1, 18)  72          ['activation_147[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_158 (Conv2D)            (None, 129, 1, 30)   2700        ['batch_normalization_147[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_19 (TFOpL  (None, 129, 1, 30)  0           ['conv2d_158[0][0]',             \n",
            " ambda)                                                           'conv2d_146[0][0]']             \n",
            "                                                                                                  \n",
            " activation_148 (Activation)    (None, 129, 1, 30)   0           ['tf.__operators__.add_19[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_148 (Batch  (None, 129, 1, 30)  120         ['activation_148[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_159 (Conv2D)            (None, 129, 1, 8)    2160        ['batch_normalization_148[0][0]']\n",
            "                                                                                                  \n",
            " activation_149 (Activation)    (None, 129, 1, 8)    0           ['conv2d_159[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_149 (Batch  (None, 129, 1, 8)   32          ['activation_149[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " spatial_dropout2d_9 (SpatialDr  (None, 129, 1, 8)   0           ['batch_normalization_149[0][0]']\n",
            " opout2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_160 (Conv2D)            (None, 129, 1, 1)    1033        ['spatial_dropout2d_9[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 32,933\n",
            "Trainable params: 32,373\n",
            "Non-trainable params: 560\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_model(l2_strength=0.0)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "8rWOgsdwglFE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
          ]
        }
      ],
      "source": [
        "# You might need to install the following dependencies: sudo apt install python-pydot python-pydot-ng graphviz\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, dpi=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "7OdmgHTp4_Ou"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6007 (pid 498806), started 1:31:32 ago. (Use '!kill 498806' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-c62ef9c2cda22c7e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-c62ef9c2cda22c7e\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6007;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "BucSAwkHQj8Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4081/4081 [==============================] - 25s 6ms/step - loss: 0.6973 - rmse: 0.8351\n",
            "Baseline accuracy 0.6973141431808472\n"
          ]
        }
      ],
      "source": [
        "baseline_val_loss = model.evaluate(test_dataset)[0]\n",
        "print(f\"Baseline accuracy {baseline_val_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "ocycFbP5-X0o"
      },
      "outputs": [],
      "source": [
        "def l2_norm(vector):\n",
        "    return np.square(vector)\n",
        "\n",
        "def SDR(denoised, cleaned, eps=1e-7): # Signal to Distortion Ratio\n",
        "    a = l2_norm(denoised)\n",
        "    b = l2_norm(denoised - cleaned)\n",
        "    a_b = a / b\n",
        "    return np.mean(10 * np.log10(a_b + eps))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "log_folder = Path(f\"./logs/{model_name}\")\n",
        "result_folder = Path(f\"./result/{model_name}\")\n",
        "\n",
        "if not log_folder.is_dir():\n",
        "    log_folder.mkdir()\n",
        "if not result_folder.is_dir():\n",
        "    result_folder.mkdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "bcPAuMZ9SlHa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "600/600 [==============================] - 31s 49ms/step - loss: 0.4195 - rmse: 0.6477 - val_loss: 0.2765 - val_rmse: 0.5259\n",
            "Epoch 2/400\n",
            "600/600 [==============================] - 31s 52ms/step - loss: 0.2632 - rmse: 0.5130 - val_loss: 0.2316 - val_rmse: 0.4812\n",
            "Epoch 3/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.2465 - rmse: 0.4965 - val_loss: 0.2232 - val_rmse: 0.4725\n",
            "Epoch 4/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.2272 - rmse: 0.4767 - val_loss: 0.2348 - val_rmse: 0.4846\n",
            "Epoch 5/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.2335 - rmse: 0.4832 - val_loss: 0.2082 - val_rmse: 0.4562\n",
            "Epoch 6/400\n",
            "600/600 [==============================] - 29s 49ms/step - loss: 0.2095 - rmse: 0.4577 - val_loss: 0.2024 - val_rmse: 0.4499\n",
            "Epoch 7/400\n",
            "600/600 [==============================] - 28s 48ms/step - loss: 0.2118 - rmse: 0.4602 - val_loss: 0.2078 - val_rmse: 0.4558\n",
            "Epoch 8/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.2061 - rmse: 0.4539 - val_loss: 0.2057 - val_rmse: 0.4535\n",
            "Epoch 9/400\n",
            "600/600 [==============================] - 30s 49ms/step - loss: 0.2024 - rmse: 0.4499 - val_loss: 0.1951 - val_rmse: 0.4417\n",
            "Epoch 10/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.2036 - rmse: 0.4512 - val_loss: 0.2157 - val_rmse: 0.4644\n",
            "Epoch 11/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1964 - rmse: 0.4432 - val_loss: 0.2079 - val_rmse: 0.4560\n",
            "Epoch 12/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1987 - rmse: 0.4458 - val_loss: 0.1949 - val_rmse: 0.4415\n",
            "Epoch 13/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1989 - rmse: 0.4460 - val_loss: 0.1968 - val_rmse: 0.4436\n",
            "Epoch 14/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.2057 - rmse: 0.4536 - val_loss: 0.2125 - val_rmse: 0.4610\n",
            "Epoch 15/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.2020 - rmse: 0.4495 - val_loss: 0.1887 - val_rmse: 0.4344\n",
            "Epoch 16/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1885 - rmse: 0.4342 - val_loss: 0.1991 - val_rmse: 0.4462\n",
            "Epoch 17/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1940 - rmse: 0.4404 - val_loss: 0.2103 - val_rmse: 0.4586\n",
            "Epoch 18/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1948 - rmse: 0.4414 - val_loss: 0.1958 - val_rmse: 0.4425\n",
            "Epoch 19/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1978 - rmse: 0.4448 - val_loss: 0.1973 - val_rmse: 0.4442\n",
            "Epoch 20/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1957 - rmse: 0.4424 - val_loss: 0.1891 - val_rmse: 0.4349\n",
            "Epoch 21/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.2056 - rmse: 0.4535 - val_loss: 0.2135 - val_rmse: 0.4620\n",
            "Epoch 22/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1854 - rmse: 0.4305 - val_loss: 0.1874 - val_rmse: 0.4329\n",
            "Epoch 23/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1919 - rmse: 0.4381 - val_loss: 0.1819 - val_rmse: 0.4265\n",
            "Epoch 24/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1873 - rmse: 0.4328 - val_loss: 0.1929 - val_rmse: 0.4392\n",
            "Epoch 25/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1861 - rmse: 0.4313 - val_loss: 0.1897 - val_rmse: 0.4355\n",
            "Epoch 26/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1891 - rmse: 0.4348 - val_loss: 0.1883 - val_rmse: 0.4340\n",
            "Epoch 27/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1804 - rmse: 0.4247 - val_loss: 0.1813 - val_rmse: 0.4258\n",
            "Epoch 28/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1873 - rmse: 0.4328 - val_loss: 0.2214 - val_rmse: 0.4705\n",
            "Epoch 29/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1879 - rmse: 0.4334 - val_loss: 0.2016 - val_rmse: 0.4490\n",
            "Epoch 30/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1946 - rmse: 0.4412 - val_loss: 0.1909 - val_rmse: 0.4369\n",
            "Epoch 31/400\n",
            "600/600 [==============================] - 31s 52ms/step - loss: 0.1898 - rmse: 0.4357 - val_loss: 0.1793 - val_rmse: 0.4235\n",
            "Epoch 32/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1821 - rmse: 0.4268 - val_loss: 0.1815 - val_rmse: 0.4260\n",
            "Epoch 33/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1852 - rmse: 0.4304 - val_loss: 0.1814 - val_rmse: 0.4259\n",
            "Epoch 34/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1892 - rmse: 0.4349 - val_loss: 0.2085 - val_rmse: 0.4566\n",
            "Epoch 35/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1924 - rmse: 0.4386 - val_loss: 0.1849 - val_rmse: 0.4300\n",
            "Epoch 36/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1888 - rmse: 0.4345 - val_loss: 0.1857 - val_rmse: 0.4310\n",
            "Epoch 37/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1987 - rmse: 0.4457 - val_loss: 0.1826 - val_rmse: 0.4273\n",
            "Epoch 38/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1803 - rmse: 0.4246 - val_loss: 0.1815 - val_rmse: 0.4260\n",
            "Epoch 39/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1841 - rmse: 0.4291 - val_loss: 0.1772 - val_rmse: 0.4209\n",
            "Epoch 40/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1841 - rmse: 0.4290 - val_loss: 0.1747 - val_rmse: 0.4180\n",
            "Epoch 41/400\n",
            "600/600 [==============================] - 29s 49ms/step - loss: 0.1845 - rmse: 0.4296 - val_loss: 0.1819 - val_rmse: 0.4264\n",
            "Epoch 42/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1809 - rmse: 0.4253 - val_loss: 0.1852 - val_rmse: 0.4304\n",
            "Epoch 43/400\n",
            "600/600 [==============================] - 29s 49ms/step - loss: 0.1787 - rmse: 0.4227 - val_loss: 0.1777 - val_rmse: 0.4215\n",
            "Epoch 44/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1820 - rmse: 0.4266 - val_loss: 0.1775 - val_rmse: 0.4213\n",
            "Epoch 45/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1842 - rmse: 0.4292 - val_loss: 0.2032 - val_rmse: 0.4508\n",
            "Epoch 46/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1901 - rmse: 0.4360 - val_loss: 0.1787 - val_rmse: 0.4227\n",
            "Epoch 47/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1851 - rmse: 0.4302 - val_loss: 0.1806 - val_rmse: 0.4250\n",
            "Epoch 48/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1790 - rmse: 0.4230 - val_loss: 0.2042 - val_rmse: 0.4518\n",
            "Epoch 49/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1863 - rmse: 0.4316 - val_loss: 0.1868 - val_rmse: 0.4322\n",
            "Epoch 50/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1826 - rmse: 0.4274 - val_loss: 0.1793 - val_rmse: 0.4234\n",
            "Epoch 51/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1886 - rmse: 0.4342 - val_loss: 0.1900 - val_rmse: 0.4359\n",
            "Epoch 52/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1876 - rmse: 0.4332 - val_loss: 0.1844 - val_rmse: 0.4294\n",
            "Epoch 53/400\n",
            "600/600 [==============================] - 25s 43ms/step - loss: 0.1901 - rmse: 0.4360 - val_loss: 0.2030 - val_rmse: 0.4506\n",
            "Epoch 54/400\n",
            "600/600 [==============================] - 24s 41ms/step - loss: 0.1797 - rmse: 0.4240 - val_loss: 0.1750 - val_rmse: 0.4184\n",
            "Epoch 55/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1796 - rmse: 0.4238 - val_loss: 0.1857 - val_rmse: 0.4310\n",
            "Epoch 56/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1835 - rmse: 0.4283 - val_loss: 0.1720 - val_rmse: 0.4147\n",
            "Epoch 57/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1805 - rmse: 0.4248 - val_loss: 0.1835 - val_rmse: 0.4283\n",
            "Epoch 58/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1768 - rmse: 0.4205 - val_loss: 0.1739 - val_rmse: 0.4170\n",
            "Epoch 59/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1787 - rmse: 0.4228 - val_loss: 0.1852 - val_rmse: 0.4304\n",
            "Epoch 60/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1793 - rmse: 0.4235 - val_loss: 0.1775 - val_rmse: 0.4213\n",
            "Epoch 61/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1824 - rmse: 0.4271 - val_loss: 0.1913 - val_rmse: 0.4374\n",
            "Epoch 62/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1858 - rmse: 0.4311 - val_loss: 0.1770 - val_rmse: 0.4208\n",
            "Epoch 63/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1844 - rmse: 0.4294 - val_loss: 0.1811 - val_rmse: 0.4256\n",
            "Epoch 64/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1756 - rmse: 0.4190 - val_loss: 0.1743 - val_rmse: 0.4175\n",
            "Epoch 65/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1840 - rmse: 0.4290 - val_loss: 0.1741 - val_rmse: 0.4172\n",
            "Epoch 66/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1850 - rmse: 0.4301 - val_loss: 0.1745 - val_rmse: 0.4177\n",
            "Epoch 67/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1829 - rmse: 0.4276 - val_loss: 0.1746 - val_rmse: 0.4179\n",
            "Epoch 68/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1886 - rmse: 0.4343 - val_loss: 0.1747 - val_rmse: 0.4180\n",
            "Epoch 69/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1853 - rmse: 0.4304 - val_loss: 0.1737 - val_rmse: 0.4168\n",
            "Epoch 70/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1782 - rmse: 0.4222 - val_loss: 0.1705 - val_rmse: 0.4129\n",
            "Epoch 71/400\n",
            "600/600 [==============================] - 30s 51ms/step - loss: 0.1774 - rmse: 0.4212 - val_loss: 0.1909 - val_rmse: 0.4369\n",
            "Epoch 72/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1806 - rmse: 0.4250 - val_loss: 0.2078 - val_rmse: 0.4559\n",
            "Epoch 73/400\n",
            "600/600 [==============================] - 29s 49ms/step - loss: 0.1795 - rmse: 0.4237 - val_loss: 0.1724 - val_rmse: 0.4152\n",
            "Epoch 74/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1761 - rmse: 0.4196 - val_loss: 0.1741 - val_rmse: 0.4173\n",
            "Epoch 75/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1749 - rmse: 0.4182 - val_loss: 0.1806 - val_rmse: 0.4250\n",
            "Epoch 76/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1765 - rmse: 0.4201 - val_loss: 0.1736 - val_rmse: 0.4166\n",
            "Epoch 77/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1829 - rmse: 0.4276 - val_loss: 0.1772 - val_rmse: 0.4209\n",
            "Epoch 78/400\n",
            "600/600 [==============================] - 30s 51ms/step - loss: 0.1847 - rmse: 0.4297 - val_loss: 0.1724 - val_rmse: 0.4152\n",
            "Epoch 79/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1796 - rmse: 0.4238 - val_loss: 0.1739 - val_rmse: 0.4170\n",
            "Epoch 80/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1754 - rmse: 0.4188 - val_loss: 0.1711 - val_rmse: 0.4136\n",
            "Epoch 81/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1824 - rmse: 0.4271 - val_loss: 0.1844 - val_rmse: 0.4294\n",
            "Epoch 82/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1875 - rmse: 0.4330 - val_loss: 0.1742 - val_rmse: 0.4173\n",
            "Epoch 83/400\n",
            "600/600 [==============================] - 31s 51ms/step - loss: 0.1777 - rmse: 0.4215 - val_loss: 0.1844 - val_rmse: 0.4294\n",
            "Epoch 84/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1869 - rmse: 0.4323 - val_loss: 0.1717 - val_rmse: 0.4143\n",
            "Epoch 85/400\n",
            "600/600 [==============================] - 30s 51ms/step - loss: 0.1827 - rmse: 0.4275 - val_loss: 0.1713 - val_rmse: 0.4139\n",
            "Epoch 86/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1783 - rmse: 0.4222 - val_loss: 0.1850 - val_rmse: 0.4301\n",
            "Epoch 87/400\n",
            "600/600 [==============================] - 29s 49ms/step - loss: 0.1746 - rmse: 0.4178 - val_loss: 0.1703 - val_rmse: 0.4126\n",
            "Epoch 88/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1772 - rmse: 0.4210 - val_loss: 0.1797 - val_rmse: 0.4239\n",
            "Epoch 89/400\n",
            "600/600 [==============================] - 29s 49ms/step - loss: 0.1767 - rmse: 0.4204 - val_loss: 0.1714 - val_rmse: 0.4140\n",
            "Epoch 90/400\n",
            "600/600 [==============================] - 31s 52ms/step - loss: 0.1739 - rmse: 0.4170 - val_loss: 0.1789 - val_rmse: 0.4230\n",
            "Epoch 91/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1752 - rmse: 0.4185 - val_loss: 0.1823 - val_rmse: 0.4270\n",
            "Epoch 92/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1748 - rmse: 0.4181 - val_loss: 0.1741 - val_rmse: 0.4173\n",
            "Epoch 93/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1811 - rmse: 0.4256 - val_loss: 0.1742 - val_rmse: 0.4173\n",
            "Epoch 94/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1849 - rmse: 0.4300 - val_loss: 0.1759 - val_rmse: 0.4194\n",
            "Epoch 95/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1756 - rmse: 0.4190 - val_loss: 0.1723 - val_rmse: 0.4151\n",
            "Epoch 96/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1748 - rmse: 0.4181 - val_loss: 0.1865 - val_rmse: 0.4318\n",
            "Epoch 97/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1816 - rmse: 0.4262 - val_loss: 0.1711 - val_rmse: 0.4136\n",
            "Epoch 98/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1859 - rmse: 0.4312 - val_loss: 0.1771 - val_rmse: 0.4209\n",
            "Epoch 99/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1778 - rmse: 0.4217 - val_loss: 0.1785 - val_rmse: 0.4225\n",
            "Epoch 100/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1890 - rmse: 0.4348 - val_loss: 0.1721 - val_rmse: 0.4149\n",
            "Epoch 101/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1760 - rmse: 0.4195 - val_loss: 0.1699 - val_rmse: 0.4122\n",
            "Epoch 102/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1775 - rmse: 0.4213 - val_loss: 0.1819 - val_rmse: 0.4265\n",
            "Epoch 103/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1738 - rmse: 0.4170 - val_loss: 0.1717 - val_rmse: 0.4143\n",
            "Epoch 104/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1755 - rmse: 0.4190 - val_loss: 0.1770 - val_rmse: 0.4207\n",
            "Epoch 105/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1757 - rmse: 0.4191 - val_loss: 0.1738 - val_rmse: 0.4168\n",
            "Epoch 106/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1720 - rmse: 0.4147 - val_loss: 0.1763 - val_rmse: 0.4198\n",
            "Epoch 107/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1745 - rmse: 0.4177 - val_loss: 0.1720 - val_rmse: 0.4147\n",
            "Epoch 108/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1765 - rmse: 0.4201 - val_loss: 0.2044 - val_rmse: 0.4522\n",
            "Epoch 109/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1819 - rmse: 0.4265 - val_loss: 0.1684 - val_rmse: 0.4103\n",
            "Epoch 110/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1824 - rmse: 0.4271 - val_loss: 0.1862 - val_rmse: 0.4315\n",
            "Epoch 111/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1727 - rmse: 0.4156 - val_loss: 0.1776 - val_rmse: 0.4215\n",
            "Epoch 112/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1748 - rmse: 0.4181 - val_loss: 0.1694 - val_rmse: 0.4115\n",
            "Epoch 113/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1784 - rmse: 0.4224 - val_loss: 0.1691 - val_rmse: 0.4112\n",
            "Epoch 114/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1860 - rmse: 0.4313 - val_loss: 0.1684 - val_rmse: 0.4103\n",
            "Epoch 115/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1777 - rmse: 0.4215 - val_loss: 0.1728 - val_rmse: 0.4157\n",
            "Epoch 116/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1883 - rmse: 0.4339 - val_loss: 0.1671 - val_rmse: 0.4088\n",
            "Epoch 117/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1742 - rmse: 0.4173 - val_loss: 0.1970 - val_rmse: 0.4439\n",
            "Epoch 118/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1766 - rmse: 0.4202 - val_loss: 0.2007 - val_rmse: 0.4480\n",
            "Epoch 119/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1731 - rmse: 0.4161 - val_loss: 0.1741 - val_rmse: 0.4172\n",
            "Epoch 120/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1739 - rmse: 0.4170 - val_loss: 0.1676 - val_rmse: 0.4094\n",
            "Epoch 121/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1759 - rmse: 0.4195 - val_loss: 0.1780 - val_rmse: 0.4218\n",
            "Epoch 122/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1702 - rmse: 0.4126 - val_loss: 0.1731 - val_rmse: 0.4161\n",
            "Epoch 123/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1731 - rmse: 0.4161 - val_loss: 0.1760 - val_rmse: 0.4195\n",
            "Epoch 124/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1758 - rmse: 0.4192 - val_loss: 0.1724 - val_rmse: 0.4152\n",
            "Epoch 125/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1823 - rmse: 0.4269 - val_loss: 0.1773 - val_rmse: 0.4211\n",
            "Epoch 126/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1834 - rmse: 0.4283 - val_loss: 0.1696 - val_rmse: 0.4118\n",
            "Epoch 127/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1681 - rmse: 0.4100 - val_loss: 0.1769 - val_rmse: 0.4206\n",
            "Epoch 128/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1762 - rmse: 0.4198 - val_loss: 0.1680 - val_rmse: 0.4098\n",
            "Epoch 129/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1763 - rmse: 0.4198 - val_loss: 0.1692 - val_rmse: 0.4113\n",
            "Epoch 130/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1824 - rmse: 0.4271 - val_loss: 0.1749 - val_rmse: 0.4182\n",
            "Epoch 131/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1804 - rmse: 0.4248 - val_loss: 0.1693 - val_rmse: 0.4114\n",
            "Epoch 132/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1882 - rmse: 0.4338 - val_loss: 0.1676 - val_rmse: 0.4094\n",
            "Epoch 133/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1709 - rmse: 0.4134 - val_loss: 0.1817 - val_rmse: 0.4262\n",
            "Epoch 134/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1776 - rmse: 0.4214 - val_loss: 0.1763 - val_rmse: 0.4199\n",
            "Epoch 135/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1721 - rmse: 0.4148 - val_loss: 0.1709 - val_rmse: 0.4135\n",
            "Epoch 136/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1733 - rmse: 0.4163 - val_loss: 0.1832 - val_rmse: 0.4281\n",
            "Epoch 137/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1746 - rmse: 0.4179 - val_loss: 0.1718 - val_rmse: 0.4145\n",
            "Epoch 138/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1685 - rmse: 0.4105 - val_loss: 0.1683 - val_rmse: 0.4102\n",
            "Epoch 139/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1746 - rmse: 0.4179 - val_loss: 0.1685 - val_rmse: 0.4105\n",
            "Epoch 140/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1739 - rmse: 0.4171 - val_loss: 0.1689 - val_rmse: 0.4110\n",
            "Epoch 141/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1816 - rmse: 0.4261 - val_loss: 0.1762 - val_rmse: 0.4198\n",
            "Epoch 142/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1804 - rmse: 0.4247 - val_loss: 0.1716 - val_rmse: 0.4142\n",
            "Epoch 143/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1681 - rmse: 0.4100 - val_loss: 0.1656 - val_rmse: 0.4069\n",
            "Epoch 144/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1751 - rmse: 0.4185 - val_loss: 0.1857 - val_rmse: 0.4310\n",
            "Epoch 145/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1766 - rmse: 0.4202 - val_loss: 0.1684 - val_rmse: 0.4104\n",
            "Epoch 146/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1802 - rmse: 0.4245 - val_loss: 0.1659 - val_rmse: 0.4073\n",
            "Epoch 147/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1785 - rmse: 0.4225 - val_loss: 0.1719 - val_rmse: 0.4146\n",
            "Epoch 148/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1895 - rmse: 0.4353 - val_loss: 0.1693 - val_rmse: 0.4115\n",
            "Epoch 149/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1690 - rmse: 0.4111 - val_loss: 0.1691 - val_rmse: 0.4112\n",
            "Epoch 150/400\n",
            "600/600 [==============================] - 25s 43ms/step - loss: 0.1760 - rmse: 0.4195 - val_loss: 0.1797 - val_rmse: 0.4239\n",
            "Epoch 151/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1725 - rmse: 0.4153 - val_loss: 0.1707 - val_rmse: 0.4132\n",
            "Epoch 152/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1724 - rmse: 0.4152 - val_loss: 0.1783 - val_rmse: 0.4223\n",
            "Epoch 153/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1751 - rmse: 0.4184 - val_loss: 0.1693 - val_rmse: 0.4115\n",
            "Epoch 154/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1667 - rmse: 0.4083 - val_loss: 0.1707 - val_rmse: 0.4132\n",
            "Epoch 155/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1730 - rmse: 0.4159 - val_loss: 0.1793 - val_rmse: 0.4234\n",
            "Epoch 156/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1747 - rmse: 0.4179 - val_loss: 0.1705 - val_rmse: 0.4129\n",
            "Epoch 157/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1811 - rmse: 0.4255 - val_loss: 0.1711 - val_rmse: 0.4136\n",
            "Epoch 158/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1776 - rmse: 0.4214 - val_loss: 0.1708 - val_rmse: 0.4133\n",
            "Epoch 159/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1699 - rmse: 0.4122 - val_loss: 0.1666 - val_rmse: 0.4081\n",
            "Epoch 160/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1768 - rmse: 0.4204 - val_loss: 0.1738 - val_rmse: 0.4168\n",
            "Epoch 161/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1733 - rmse: 0.4163 - val_loss: 0.1663 - val_rmse: 0.4078\n",
            "Epoch 162/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1803 - rmse: 0.4247 - val_loss: 0.1647 - val_rmse: 0.4058\n",
            "Epoch 163/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1790 - rmse: 0.4231 - val_loss: 0.1692 - val_rmse: 0.4114\n",
            "Epoch 164/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1845 - rmse: 0.4295 - val_loss: 0.1690 - val_rmse: 0.4111\n",
            "Epoch 165/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1714 - rmse: 0.4140 - val_loss: 0.1831 - val_rmse: 0.4279\n",
            "Epoch 166/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1721 - rmse: 0.4148 - val_loss: 0.1664 - val_rmse: 0.4079\n",
            "Epoch 167/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1744 - rmse: 0.4176 - val_loss: 0.1674 - val_rmse: 0.4092\n",
            "Epoch 168/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1739 - rmse: 0.4170 - val_loss: 0.1679 - val_rmse: 0.4097\n",
            "Epoch 169/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1696 - rmse: 0.4118 - val_loss: 0.1908 - val_rmse: 0.4369\n",
            "Epoch 170/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1696 - rmse: 0.4118 - val_loss: 0.1691 - val_rmse: 0.4113\n",
            "Epoch 171/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1728 - rmse: 0.4157 - val_loss: 0.1711 - val_rmse: 0.4137\n",
            "Epoch 172/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1741 - rmse: 0.4172 - val_loss: 0.1673 - val_rmse: 0.4090\n",
            "Epoch 173/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1791 - rmse: 0.4232 - val_loss: 0.1729 - val_rmse: 0.4158\n",
            "Epoch 174/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1765 - rmse: 0.4201 - val_loss: 0.1634 - val_rmse: 0.4042\n",
            "Epoch 175/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1694 - rmse: 0.4116 - val_loss: 0.1675 - val_rmse: 0.4092\n",
            "Epoch 176/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1769 - rmse: 0.4206 - val_loss: 0.1894 - val_rmse: 0.4352\n",
            "Epoch 177/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1745 - rmse: 0.4178 - val_loss: 0.1700 - val_rmse: 0.4123\n",
            "Epoch 178/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1790 - rmse: 0.4231 - val_loss: 0.1666 - val_rmse: 0.4082\n",
            "Epoch 179/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1794 - rmse: 0.4236 - val_loss: 0.1759 - val_rmse: 0.4195\n",
            "Epoch 180/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1810 - rmse: 0.4255 - val_loss: 0.1795 - val_rmse: 0.4237\n",
            "Epoch 181/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1715 - rmse: 0.4141 - val_loss: 0.1728 - val_rmse: 0.4157\n",
            "Epoch 182/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1717 - rmse: 0.4144 - val_loss: 0.1843 - val_rmse: 0.4293\n",
            "Epoch 183/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1744 - rmse: 0.4177 - val_loss: 0.1656 - val_rmse: 0.4069\n",
            "Epoch 184/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1737 - rmse: 0.4167 - val_loss: 0.2153 - val_rmse: 0.4640\n",
            "Epoch 185/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1694 - rmse: 0.4116 - val_loss: 0.1682 - val_rmse: 0.4101\n",
            "Epoch 186/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1688 - rmse: 0.4108 - val_loss: 0.1676 - val_rmse: 0.4093\n",
            "Epoch 187/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1713 - rmse: 0.4139 - val_loss: 0.1656 - val_rmse: 0.4069\n",
            "Epoch 188/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1747 - rmse: 0.4180 - val_loss: 0.1653 - val_rmse: 0.4065\n",
            "Epoch 189/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1773 - rmse: 0.4211 - val_loss: 0.1651 - val_rmse: 0.4063\n",
            "Epoch 190/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1748 - rmse: 0.4181 - val_loss: 0.1747 - val_rmse: 0.4180\n",
            "Epoch 191/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1684 - rmse: 0.4103 - val_loss: 0.1762 - val_rmse: 0.4198\n",
            "Epoch 192/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1770 - rmse: 0.4208 - val_loss: 0.1730 - val_rmse: 0.4159\n",
            "Epoch 193/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1788 - rmse: 0.4229 - val_loss: 0.1663 - val_rmse: 0.4078\n",
            "Epoch 194/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1748 - rmse: 0.4181 - val_loss: 0.1715 - val_rmse: 0.4142\n",
            "Epoch 195/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1814 - rmse: 0.4259 - val_loss: 0.1690 - val_rmse: 0.4111\n",
            "Epoch 196/400\n",
            "600/600 [==============================] - 25s 43ms/step - loss: 0.1792 - rmse: 0.4233 - val_loss: 0.1805 - val_rmse: 0.4249\n",
            "Epoch 197/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1708 - rmse: 0.4133 - val_loss: 0.1645 - val_rmse: 0.4056\n",
            "Epoch 198/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1710 - rmse: 0.4135 - val_loss: 0.1789 - val_rmse: 0.4229\n",
            "Epoch 199/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1719 - rmse: 0.4146 - val_loss: 0.1636 - val_rmse: 0.4045\n",
            "Epoch 200/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1721 - rmse: 0.4148 - val_loss: 0.1686 - val_rmse: 0.4106\n",
            "Epoch 201/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1703 - rmse: 0.4126 - val_loss: 0.1708 - val_rmse: 0.4133\n",
            "Epoch 202/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1688 - rmse: 0.4108 - val_loss: 0.1744 - val_rmse: 0.4176\n",
            "Epoch 203/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1684 - rmse: 0.4104 - val_loss: 0.1701 - val_rmse: 0.4124\n",
            "Epoch 204/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1764 - rmse: 0.4200 - val_loss: 0.1670 - val_rmse: 0.4086\n",
            "Epoch 205/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1788 - rmse: 0.4228 - val_loss: 0.1862 - val_rmse: 0.4315\n",
            "Epoch 206/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1732 - rmse: 0.4161 - val_loss: 0.1659 - val_rmse: 0.4073\n",
            "Epoch 207/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1694 - rmse: 0.4116 - val_loss: 0.1673 - val_rmse: 0.4090\n",
            "Epoch 208/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1763 - rmse: 0.4199 - val_loss: 0.1774 - val_rmse: 0.4212\n",
            "Epoch 209/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1817 - rmse: 0.4262 - val_loss: 0.1653 - val_rmse: 0.4065\n",
            "Epoch 210/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1705 - rmse: 0.4129 - val_loss: 0.1756 - val_rmse: 0.4191\n",
            "Epoch 211/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1819 - rmse: 0.4265 - val_loss: 0.1890 - val_rmse: 0.4347\n",
            "Epoch 212/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1757 - rmse: 0.4191 - val_loss: 0.1657 - val_rmse: 0.4071\n",
            "Epoch 213/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1730 - rmse: 0.4160 - val_loss: 0.1761 - val_rmse: 0.4196\n",
            "Epoch 214/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1697 - rmse: 0.4120 - val_loss: 0.1666 - val_rmse: 0.4082\n",
            "Epoch 215/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1704 - rmse: 0.4128 - val_loss: 0.1629 - val_rmse: 0.4036\n",
            "Epoch 216/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1713 - rmse: 0.4139 - val_loss: 0.1673 - val_rmse: 0.4090\n",
            "Epoch 217/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1679 - rmse: 0.4097 - val_loss: 0.1686 - val_rmse: 0.4106\n",
            "Epoch 218/400\n",
            "600/600 [==============================] - 30s 49ms/step - loss: 0.1697 - rmse: 0.4119 - val_loss: 0.1748 - val_rmse: 0.4181\n",
            "Epoch 219/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1696 - rmse: 0.4118 - val_loss: 0.1686 - val_rmse: 0.4106\n",
            "Epoch 220/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1765 - rmse: 0.4202 - val_loss: 0.1656 - val_rmse: 0.4069\n",
            "Epoch 221/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1791 - rmse: 0.4232 - val_loss: 0.1684 - val_rmse: 0.4104\n",
            "Epoch 222/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1696 - rmse: 0.4118 - val_loss: 0.1662 - val_rmse: 0.4077\n",
            "Epoch 223/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1704 - rmse: 0.4128 - val_loss: 0.1644 - val_rmse: 0.4055\n",
            "Epoch 224/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1756 - rmse: 0.4190 - val_loss: 0.1627 - val_rmse: 0.4034\n",
            "Epoch 225/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1805 - rmse: 0.4248 - val_loss: 0.1846 - val_rmse: 0.4296\n",
            "Epoch 226/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1730 - rmse: 0.4159 - val_loss: 0.1647 - val_rmse: 0.4059\n",
            "Epoch 227/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1855 - rmse: 0.4307 - val_loss: 0.1827 - val_rmse: 0.4274\n",
            "Epoch 228/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1689 - rmse: 0.4109 - val_loss: 0.1753 - val_rmse: 0.4187\n",
            "Epoch 229/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1723 - rmse: 0.4151 - val_loss: 0.1702 - val_rmse: 0.4125\n",
            "Epoch 230/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1690 - rmse: 0.4111 - val_loss: 0.1753 - val_rmse: 0.4187\n",
            "Epoch 231/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1701 - rmse: 0.4124 - val_loss: 0.1619 - val_rmse: 0.4023\n",
            "Epoch 232/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1723 - rmse: 0.4151 - val_loss: 0.1659 - val_rmse: 0.4073\n",
            "Epoch 233/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1653 - rmse: 0.4066 - val_loss: 0.1625 - val_rmse: 0.4031\n",
            "Epoch 234/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1700 - rmse: 0.4123 - val_loss: 0.1637 - val_rmse: 0.4046\n",
            "Epoch 235/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1716 - rmse: 0.4142 - val_loss: 0.1671 - val_rmse: 0.4088\n",
            "Epoch 236/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1763 - rmse: 0.4198 - val_loss: 0.1679 - val_rmse: 0.4097\n",
            "Epoch 237/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1787 - rmse: 0.4227 - val_loss: 0.1650 - val_rmse: 0.4062\n",
            "Epoch 238/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1677 - rmse: 0.4095 - val_loss: 0.1670 - val_rmse: 0.4086\n",
            "Epoch 239/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1699 - rmse: 0.4122 - val_loss: 0.1768 - val_rmse: 0.4205\n",
            "Epoch 240/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1737 - rmse: 0.4168 - val_loss: 0.1683 - val_rmse: 0.4103\n",
            "Epoch 241/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1799 - rmse: 0.4242 - val_loss: 0.1731 - val_rmse: 0.4161\n",
            "Epoch 242/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1729 - rmse: 0.4158 - val_loss: 0.1694 - val_rmse: 0.4116\n",
            "Epoch 243/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1857 - rmse: 0.4309 - val_loss: 0.1659 - val_rmse: 0.4074\n",
            "Epoch 244/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1682 - rmse: 0.4101 - val_loss: 0.1659 - val_rmse: 0.4073\n",
            "Epoch 245/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1732 - rmse: 0.4161 - val_loss: 0.1637 - val_rmse: 0.4046\n",
            "Epoch 246/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1690 - rmse: 0.4111 - val_loss: 0.1628 - val_rmse: 0.4035\n",
            "Epoch 247/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1697 - rmse: 0.4119 - val_loss: 0.1746 - val_rmse: 0.4178\n",
            "Epoch 248/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1705 - rmse: 0.4129 - val_loss: 0.1671 - val_rmse: 0.4087\n",
            "Epoch 249/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1655 - rmse: 0.4068 - val_loss: 0.1718 - val_rmse: 0.4144\n",
            "Epoch 250/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1689 - rmse: 0.4109 - val_loss: 0.1735 - val_rmse: 0.4166\n",
            "Epoch 251/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1706 - rmse: 0.4131 - val_loss: 0.1699 - val_rmse: 0.4121\n",
            "Epoch 252/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1794 - rmse: 0.4235 - val_loss: 0.1639 - val_rmse: 0.4048\n",
            "Epoch 253/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1776 - rmse: 0.4214 - val_loss: 0.1786 - val_rmse: 0.4227\n",
            "Epoch 254/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1648 - rmse: 0.4060 - val_loss: 0.1736 - val_rmse: 0.4167\n",
            "Epoch 255/400\n",
            "600/600 [==============================] - 24s 41ms/step - loss: 0.1726 - rmse: 0.4154 - val_loss: 0.1639 - val_rmse: 0.4049\n",
            "Epoch 256/400\n",
            "600/600 [==============================] - 25s 43ms/step - loss: 0.1717 - rmse: 0.4143 - val_loss: 0.1641 - val_rmse: 0.4051\n",
            "Epoch 257/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1787 - rmse: 0.4227 - val_loss: 0.1645 - val_rmse: 0.4056\n",
            "Epoch 258/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1749 - rmse: 0.4182 - val_loss: 0.1660 - val_rmse: 0.4074\n",
            "Epoch 259/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1848 - rmse: 0.4299 - val_loss: 0.1646 - val_rmse: 0.4057\n",
            "Epoch 260/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1670 - rmse: 0.4086 - val_loss: 0.1698 - val_rmse: 0.4121\n",
            "Epoch 261/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1727 - rmse: 0.4155 - val_loss: 0.1660 - val_rmse: 0.4074\n",
            "Epoch 262/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1683 - rmse: 0.4103 - val_loss: 0.1648 - val_rmse: 0.4060\n",
            "Epoch 263/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1697 - rmse: 0.4120 - val_loss: 0.1659 - val_rmse: 0.4073\n",
            "Epoch 264/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1714 - rmse: 0.4140 - val_loss: 0.1672 - val_rmse: 0.4090\n",
            "Epoch 265/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1647 - rmse: 0.4059 - val_loss: 0.1689 - val_rmse: 0.4110\n",
            "Epoch 266/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1705 - rmse: 0.4129 - val_loss: 0.1637 - val_rmse: 0.4046\n",
            "Epoch 267/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1704 - rmse: 0.4129 - val_loss: 0.1643 - val_rmse: 0.4053\n",
            "Epoch 268/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1770 - rmse: 0.4207 - val_loss: 0.1625 - val_rmse: 0.4031\n",
            "Epoch 269/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1760 - rmse: 0.4195 - val_loss: 0.1659 - val_rmse: 0.4074\n",
            "Epoch 270/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1662 - rmse: 0.4077 - val_loss: 0.1832 - val_rmse: 0.4281\n",
            "Epoch 271/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1711 - rmse: 0.4137 - val_loss: 0.1631 - val_rmse: 0.4038\n",
            "Epoch 272/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1740 - rmse: 0.4172 - val_loss: 0.1725 - val_rmse: 0.4154\n",
            "Epoch 273/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1780 - rmse: 0.4219 - val_loss: 0.1812 - val_rmse: 0.4257\n",
            "Epoch 274/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1746 - rmse: 0.4178 - val_loss: 0.1699 - val_rmse: 0.4122\n",
            "Epoch 275/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1850 - rmse: 0.4301 - val_loss: 0.1729 - val_rmse: 0.4158\n",
            "Epoch 276/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1653 - rmse: 0.4066 - val_loss: 0.1651 - val_rmse: 0.4063\n",
            "Epoch 277/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1712 - rmse: 0.4138 - val_loss: 0.1659 - val_rmse: 0.4073\n",
            "Epoch 278/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1694 - rmse: 0.4116 - val_loss: 0.1667 - val_rmse: 0.4083\n",
            "Epoch 279/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1713 - rmse: 0.4138 - val_loss: 0.1665 - val_rmse: 0.4080\n",
            "Epoch 280/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1684 - rmse: 0.4104 - val_loss: 0.1622 - val_rmse: 0.4028\n",
            "Epoch 281/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1640 - rmse: 0.4050 - val_loss: 0.1742 - val_rmse: 0.4174\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0cb05badf0>"
            ]
          },
          "execution_count": 201,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, baseline=None)\n",
        "logdir = os.path.join(f\"./logs/{model_name}\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch')\n",
        "\n",
        "save_path = f'./result/{model_name}/checkpoint' + \"/model-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, \n",
        "                                                         monitor='val_loss', save_best_only=True)\n",
        "\n",
        "\n",
        "model.fit(train_dataset,\n",
        "         steps_per_epoch=600, # you might need to change this\n",
        "         validation_data=test_dataset,\n",
        "         epochs=400,\n",
        "         callbacks=[early_stopping_callback, tensorboard_callback, checkpoint_callback]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "g1ZZskDZ5L2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4081/4081 [==============================] - 27s 7ms/step - loss: 0.1619 - rmse: 0.4023\n",
            "New model saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./model/assets\n"
          ]
        }
      ],
      "source": [
        "import keras.models\n",
        "val_loss = model.evaluate(test_dataset)[0]\n",
        "if val_loss < baseline_val_loss:\n",
        "  print(\"New model saved.\")\n",
        "  keras.models.save_model(model, f'.result/{model_name}/model', overwrite=True, include_optimizer=True)\n",
        "  # model.save('./denoiser_cnn_log_mel_generator.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeJTsGxCSuhm"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLiTiK8blE0n"
      },
      "outputs": [],
      "source": [
        "def read_audio(filepath, sample_rate, normalize=True):\n",
        "    \"\"\"Read an audio file and return it as a numpy array\"\"\"\n",
        "    audio, sr = librosa.load(filepath, sr=sample_rate)\n",
        "    if normalize:\n",
        "      div_fac = 1 / np.max(np.abs(audio)) / 3.0\n",
        "      audio = audio * div_fac\n",
        "    return audio, sr\n",
        "        \n",
        "def add_noise_to_clean_audio(clean_audio, noise_signal):\n",
        "    \"\"\"Adds noise to an audio sample\"\"\"\n",
        "    if len(clean_audio) >= len(noise_signal):\n",
        "        # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n",
        "        while len(clean_audio) >= len(noise_signal):\n",
        "            noise_signal = np.append(noise_signal, noise_signal)\n",
        "\n",
        "    ## Extract a noise segment from a random location in the noise file\n",
        "    ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n",
        "\n",
        "    noiseSegment = noise_signal[ind: ind + clean_audio.size]\n",
        "\n",
        "    speech_power = np.sum(clean_audio ** 2)\n",
        "    noise_power = np.sum(noiseSegment ** 2)\n",
        "    noisyAudio = clean_audio + np.sqrt(speech_power / noise_power) * noiseSegment\n",
        "    return noisyAudio\n",
        "\n",
        "def play(audio, sample_rate):\n",
        "    ipd.display(ipd.Audio(data=audio, rate=sample_rate))  # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM6ajbBFlx3b"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor:\n",
        "    def __init__(self, audio, *, windowLength, overlap, sample_rate):\n",
        "        self.audio = audio\n",
        "        self.ffT_length = windowLength\n",
        "        self.window_length = windowLength\n",
        "        self.overlap = overlap\n",
        "        self.sample_rate = sample_rate\n",
        "        self.window = scipy.signal.hamming(self.window_length, sym=False)\n",
        "\n",
        "    def get_stft_spectrogram(self):\n",
        "        return librosa.stft(self.audio, n_fft=self.ffT_length, win_length=self.window_length, hop_length=self.overlap,\n",
        "                            window=self.window, center=True)\n",
        "\n",
        "    def get_audio_from_stft_spectrogram(self, stft_features):\n",
        "        return librosa.istft(stft_features, win_length=self.window_length, hop_length=self.overlap,\n",
        "                             window=self.window, center=True)\n",
        "\n",
        "    def get_mel_spectrogram(self):\n",
        "        return librosa.feature.melspectrogram(self.audio, sr=self.sample_rate, power=2.0, pad_mode='reflect',\n",
        "                                           n_fft=self.ffT_length, hop_length=self.overlap, center=True)\n",
        "\n",
        "    def get_audio_from_mel_spectrogram(self, M):\n",
        "        return librosa.feature.inverse.mel_to_audio(M, sr=self.sample_rate, n_fft=self.ffT_length, hop_length=self.overlap,\n",
        "                                             win_length=self.window_length, window=self.window,\n",
        "                                             center=True, pad_mode='reflect', power=2.0, n_iter=32, length=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dcnyvquSoLs"
      },
      "outputs": [],
      "source": [
        "cleanAudio, sr = read_audio(os.path.join(mozilla_basepath, 'test', 'common_voice_en_16526.mp3'), sample_rate=fs)\n",
        "print(\"Min:\", np.min(cleanAudio),\"Max:\",np.max(cleanAudio))\n",
        "ipd.Audio(data=cleanAudio, rate=sr) # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaHe1okPTvV-"
      },
      "outputs": [],
      "source": [
        "noiseAudio, sr = read_audio(os.path.join(UrbanSound8K_basepath, 'test', '7913-3-0-0.wav'), sample_rate=fs)\n",
        "print(\"Min:\", np.min(noiseAudio),\"Max:\",np.max(noiseAudio))\n",
        "ipd.Audio(data=noiseAudio, rate=sr) # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu_eOKRfTHbp"
      },
      "outputs": [],
      "source": [
        "cleanAudioFeatureExtractor = FeatureExtractor(cleanAudio, windowLength=windowLength, overlap=overlap, sample_rate=sr)\n",
        "stft_features = cleanAudioFeatureExtractor.get_stft_spectrogram()\n",
        "stft_features = np.abs(stft_features)\n",
        "print(\"Min:\", np.min(stft_features),\"Max:\",np.max(stft_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHWcmobyTP4E"
      },
      "outputs": [],
      "source": [
        "noisyAudio = add_noise_to_clean_audio(cleanAudio, noiseAudio)\n",
        "ipd.Audio(data=noisyAudio, rate=fs) # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75M29dl3bBeF"
      },
      "outputs": [],
      "source": [
        "def prepare_input_features(stft_features):\n",
        "    # Phase Aware Scaling: To avoid extreme differences (more than\n",
        "    # 45 degree) between the noisy and clean phase, the clean spectral magnitude was encoded as similar to [21]:\n",
        "    noisySTFT = np.concatenate([stft_features[:,0:numSegments-1], stft_features], axis=1)\n",
        "    stftSegments = np.zeros((numFeatures, numSegments , noisySTFT.shape[1] - numSegments + 1))\n",
        "\n",
        "    for index in range(noisySTFT.shape[1] - numSegments + 1):\n",
        "        stftSegments[:,:,index] = noisySTFT[:,index:index + numSegments]\n",
        "    return stftSegments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cjO5-cjTP6t"
      },
      "outputs": [],
      "source": [
        "noiseAudioFeatureExtractor = FeatureExtractor(noisyAudio, windowLength=windowLength, overlap=overlap, sample_rate=sr)\n",
        "noise_stft_features = noiseAudioFeatureExtractor.get_stft_spectrogram()\n",
        "\n",
        "# Paper: Besides, spectral phase was not used in the training phase.\n",
        "# At reconstruction, noisy spectral phase was used instead to\n",
        "# perform in- verse STFT and recover human speech.\n",
        "noisyPhase = np.angle(noise_stft_features)\n",
        "print(noisyPhase.shape)\n",
        "noise_stft_features = np.abs(noise_stft_features)\n",
        "\n",
        "mean = np.mean(noise_stft_features)\n",
        "std = np.std(noise_stft_features)\n",
        "noise_stft_features = (noise_stft_features - mean) / std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3p5PWWkrlE3m"
      },
      "outputs": [],
      "source": [
        "predictors = prepare_input_features(noise_stft_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pSoOw2fTP9N"
      },
      "outputs": [],
      "source": [
        "predictors = np.reshape(predictors, (predictors.shape[0], predictors.shape[1], 1, predictors.shape[2]))\n",
        "predictors = np.transpose(predictors, (3, 0, 1, 2)).astype(np.float32)\n",
        "print('predictors.shape:', predictors.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5sNR4sSTP_1"
      },
      "outputs": [],
      "source": [
        "STFTFullyConvolutional = model.predict(predictors)\n",
        "print(STFTFullyConvolutional.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzCga3PdUVwG"
      },
      "outputs": [],
      "source": [
        "def revert_features_to_audio(features, phase, cleanMean=None, cleanStd=None):\n",
        "    # scale the outpus back to the original range\n",
        "    if cleanMean and cleanStd:\n",
        "        features = cleanStd * features + cleanMean\n",
        "\n",
        "    phase = np.transpose(phase, (1, 0))\n",
        "    features = np.squeeze(features)\n",
        "\n",
        "    # features = librosa.db_to_power(features)\n",
        "    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n",
        "\n",
        "    features = np.transpose(features, (1, 0))\n",
        "    return noiseAudioFeatureExtractor.get_audio_from_stft_spectrogram(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWlUDtPzURlQ"
      },
      "outputs": [],
      "source": [
        "denoisedAudioFullyConvolutional = revert_features_to_audio(STFTFullyConvolutional, noisyPhase, mean, std)\n",
        "print(\"Min:\", np.min(denoisedAudioFullyConvolutional),\"Max:\",np.max(denoisedAudioFullyConvolutional))\n",
        "ipd.Audio(data=denoisedAudioFullyConvolutional, rate=fs) # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvEIBy7EHgeP"
      },
      "outputs": [],
      "source": [
        "# A numeric identifier of the sound class -- Types of noise\n",
        "# 0 = air_conditioner\n",
        "# 1 = car_horn\n",
        "# 2 = children_playing\n",
        "# 3 = dog_bark\n",
        "# 4 = drilling\n",
        "# 5 = engine_idling\n",
        "# 6 = gun_shot\n",
        "# 7 = jackhammer\n",
        "# 8 = siren\n",
        "# 9 = street_music"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv_7ZwWaUW0_"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2, ax3) = plt.subplots(3, 1, sharey=True)\n",
        "\n",
        "ax1.plot(cleanAudio)\n",
        "ax1.set_title(\"Clean Audio\")\n",
        "\n",
        "ax2.plot(noisyAudio)\n",
        "ax2.set_title(\"Noisy Audio\")\n",
        "\n",
        "ax3.plot(denoisedAudioFullyConvolutional)\n",
        "ax3.set_title(\"Denoised Audio\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LM7E91avKA3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "SpeechDenoiserCNN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 ('tf_29_daniel')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "904f93ba5a280e572257a04f19a20f81b654f882c045508efe9071ed46139c46"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
