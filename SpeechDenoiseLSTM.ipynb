{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VoyZ5yLRaAeI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0Metal device set to: Apple M1 Pro\n",
            "\n",
            "systemMemory: 32.00 GB\n",
            "maxCacheSize: 10.67 GB\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-09-06 11:15:23.569380: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2022-09-06 11:15:23.569501: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ah7ISzvtEmur"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import IPython.display as ipd\n",
        "import librosa.display\n",
        "import scipy\n",
        "import glob\n",
        "import numpy as np\n",
        "import math\n",
        "import warnings\n",
        "import pickle\n",
        "from sklearn.utils import shuffle\n",
        "import zipfile\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WBZyUMpobR-Z"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-09-06 11:15:24.505356: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2022-09-06 11:15:24.505376: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 10661001427947599241\n",
              " xla_global_id: -1,\n",
              " name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " locality {\n",
              "   bus_id: 1\n",
              " }\n",
              " incarnation: 11684116821122786269\n",
              " physical_device_desc: \"device: 0, name: METAL, pci bus id: <undefined>\"\n",
              " xla_global_id: -1]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zu2lgR8lEmu0"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(999)\n",
        "np.random.seed(999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JGbkvQHxxrj_"
      },
      "outputs": [],
      "source": [
        "# !wget 'cdn.daitan.com/dataset.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "51o6Ze9hxrkE"
      },
      "outputs": [],
      "source": [
        "# dataset_file_name = './dataset.zip'\n",
        "# with zipfile.ZipFile(dataset_file_name, 'r') as zip_ref:\n",
        "#     zip_ref.extractall('./dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"cnn\"\n",
        "model_name = \"lstm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d43UvhEqxrkJ"
      },
      "outputs": [],
      "source": [
        "# path_to_dataset = \"./dataset/tfrecords\"\n",
        "path_to_dataset = f\"./records_{model_name}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W36WF4mT1kGK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training file names:  []\n",
            "Validation file names:  []\n"
          ]
        }
      ],
      "source": [
        "# get training and validation tf record file names\n",
        "train_tfrecords_filenames = glob.glob(os.path.join(path_to_dataset, 'train_*'))\n",
        "val_tfrecords_filenames = glob.glob(os.path.join(path_to_dataset, 'val_*'))\n",
        "\n",
        "# suffle the file names for training\n",
        "np.random.shuffle(train_tfrecords_filenames)\n",
        "print(\"Training file names: \", train_tfrecords_filenames)\n",
        "print(\"Validation file names: \", val_tfrecords_filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jKZtPoLMEmvJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "windowLength: 512\n",
            "overlap: 256\n",
            "ffTLength: 512\n",
            "inputFs: 48000.0\n",
            "fs: 16000.0\n",
            "numFeatures: 257\n",
            "numSegments: 63\n"
          ]
        }
      ],
      "source": [
        "# lstm\n",
        "windowLength = 512\n",
        "overlap      = round(0.5 * windowLength) # overlap of 75%\n",
        "ffTLength    = windowLength\n",
        "inputFs      = 48e3\n",
        "fs           = 16e3\n",
        "numFeatures  = ffTLength//2 + 1\n",
        "numSegments  = 63 # 1 sec in 512 window, 256 hop, sr = 16000 Hz\n",
        "\n",
        "print(\"windowLength:\",windowLength)\n",
        "print(\"overlap:\",overlap)\n",
        "print(\"ffTLength:\",ffTLength)\n",
        "print(\"inputFs:\",inputFs)\n",
        "print(\"fs:\",fs)\n",
        "print(\"numFeatures:\",numFeatures)\n",
        "print(\"numSegments:\",numSegments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JXpSGe8L0cl_"
      },
      "outputs": [],
      "source": [
        "mozilla_basepath = \"./dataset/en\"\n",
        "UrbanSound8K_basepath = './dataset/UrbanSound8K'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzbdfIi-Lgk9"
      },
      "source": [
        "## Prepare Input features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "24kph3wJ3s5V"
      },
      "outputs": [],
      "source": [
        "def tf_record_parser(record):\n",
        "    keys_to_features = {\n",
        "        \"noise_stft_phase\": tf.io.FixedLenFeature((), tf.string, default_value=\"\"),\n",
        "        'noise_stft_mag_features': tf.io.FixedLenFeature([], tf.string),\n",
        "        \"clean_stft_magnitude\": tf.io.FixedLenFeature((), tf.string)\n",
        "    }\n",
        "\n",
        "    features = tf.io.parse_single_example(record, keys_to_features)\n",
        "\n",
        "    # reshape input and annotation images, lstm\n",
        "    noise_stft_mag_features = tf.reshape(noise_stft_mag_features, (1, numSegments, numFeatures), name=\"noise_stft_mag_features\")\n",
        "    clean_stft_magnitude = tf.reshape(clean_stft_magnitude, (1, numSegments, numFeatures), name=\"clean_stft_magnitude\")\n",
        "    noise_stft_phase = tf.reshape(noise_stft_phase, (numFeatures,), name=\"noise_stft_phase\")\n",
        "\n",
        "    return noise_stft_mag_features, clean_stft_magnitude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJXPGrgVTCbZ"
      },
      "source": [
        "## Create tf.Data.Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FF_A3YbZTCsj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-09-06 11:15:25.433699: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2022-09-06 11:15:25.433723: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "in user code:\n\n    File \"/var/folders/1g/912b41091855kd9w5h7bz27c0000gn/T/ipykernel_10242/2390742533.py\", line 11, in tf_record_parser  *\n        noise_stft_mag_features = tf.reshape(noise_stft_mag_features, (1, numSegments, numFeatures), name=\"noise_stft_mag_features\")\n\n    UnboundLocalError: local variable 'noise_stft_mag_features' referenced before assignment\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/seunghyunoh/workplace/study/lstm-example/cnn-audio-denoiser-voicebank/SpeechDenoiseLSTM.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/seunghyunoh/workplace/study/lstm-example/cnn-audio-denoiser-voicebank/SpeechDenoiseLSTM.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mTFRecordDataset([train_tfrecords_filenames])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/seunghyunoh/workplace/study/lstm-example/cnn-audio-denoiser-voicebank/SpeechDenoiseLSTM.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39;49mmap(tf_record_parser)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/seunghyunoh/workplace/study/lstm-example/cnn-audio-denoiser-voicebank/SpeechDenoiseLSTM.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39mshuffle(\u001b[39m8192\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/seunghyunoh/workplace/study/lstm-example/cnn-audio-denoiser-voicebank/SpeechDenoiseLSTM.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39mrepeat()\n",
            "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:2048\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2045\u001b[0m   \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m DEBUG_MODE:\n\u001b[1;32m   2046\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2047\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m`num_parallel_calls` argument is specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2048\u001b[0m   \u001b[39mreturn\u001b[39;00m MapDataset(\u001b[39mself\u001b[39;49m, map_func, preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   2049\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m   \u001b[39mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[1;32m   2051\u001b[0m       \u001b[39mself\u001b[39m,\n\u001b[1;32m   2052\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2055\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2056\u001b[0m       name\u001b[39m=\u001b[39mname)\n",
            "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:5243\u001b[0m, in \u001b[0;36mMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m   5242\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preserve_cardinality \u001b[39m=\u001b[39m preserve_cardinality\n\u001b[0;32m-> 5243\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[1;32m   5244\u001b[0m     map_func,\n\u001b[1;32m   5245\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n\u001b[1;32m   5246\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n\u001b[1;32m   5247\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n\u001b[1;32m   5248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[1;32m   5249\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mmap_dataset(\n\u001b[1;32m   5250\u001b[0m     input_dataset\u001b[39m.\u001b[39m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   5251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39mfunction\u001b[39m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5254\u001b[0m     preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preserve_cardinality,\n\u001b[1;32m   5255\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_common_args)\n",
            "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    265\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[1;32m    272\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
            "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2567\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2559\u001b[0m   \u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m   2560\u001b[0m \n\u001b[1;32m   2561\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2565\u001b[0m \u001b[39m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m   2566\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2567\u001b[0m   graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\n\u001b[1;32m   2568\u001b[0m       \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2569\u001b[0m   graph_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   2570\u001b[0m   \u001b[39mreturn\u001b[39;00m graph_function\n",
            "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2533\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2531\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m-> 2533\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m   2534\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m   2535\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m   2536\u001b[0m       graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
            "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2711\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2708\u001b[0m   cache_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mgeneralize(cache_key)\n\u001b[1;32m   2709\u001b[0m   (args, kwargs) \u001b[39m=\u001b[39m cache_key\u001b[39m.\u001b[39m_placeholder_value()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 2711\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[1;32m   2712\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39madd(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   2713\u001b[0m                          graph_function)\n\u001b[1;32m   2715\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
            "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2627\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2622\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[1;32m   2623\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   2624\u001b[0m ]\n\u001b[1;32m   2625\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[1;32m   2626\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 2627\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m   2628\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m   2629\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m   2630\u001b[0m         args,\n\u001b[1;32m   2631\u001b[0m         kwargs,\n\u001b[1;32m   2632\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[1;32m   2633\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m   2634\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m   2635\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m   2636\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[1;32m   2637\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m   2638\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m   2639\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   2640\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   2641\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   2642\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   2643\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   2644\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
            "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1141\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1139\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1141\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1143\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m   1146\u001b[0m     convert, func_outputs, expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py:248\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39m@eager_function\u001b[39m\u001b[39m.\u001b[39mdefun_with_attributes(\n\u001b[1;32m    243\u001b[0m     input_signature\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_specs(\n\u001b[1;32m    244\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_structure),\n\u001b[1;32m    245\u001b[0m     autograph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m     attributes\u001b[39m=\u001b[39mdefun_kwargs)\n\u001b[1;32m    247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m   ret \u001b[39m=\u001b[39m wrapper_helper(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    249\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    250\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
            "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py:177\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    176\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 177\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39;49mtf_convert(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func, ag_ctx)(\u001b[39m*\u001b[39;49mnested_args)\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m _should_pack(ret):\n\u001b[1;32m    179\u001b[0m   ret \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(ret)\n",
            "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m    693\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
            "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
            "File \u001b[0;32m/var/folders/1g/912b41091855kd9w5h7bz27c0000gn/T/__autograph_generated_filehico1j7t.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__tf_record_parser\u001b[0;34m(record)\u001b[0m\n\u001b[1;32m     10\u001b[0m keys_to_features \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mnoise_stft_phase\u001b[39m\u001b[39m'\u001b[39m: ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mFixedLenFeature, ((), ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mstring), \u001b[39mdict\u001b[39m(default_value\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m), fscope), \u001b[39m'\u001b[39m\u001b[39mnoise_stft_mag_features\u001b[39m\u001b[39m'\u001b[39m: ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mFixedLenFeature, ([], ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mstring), \u001b[39mNone\u001b[39;00m, fscope), \u001b[39m'\u001b[39m\u001b[39mclean_stft_magnitude\u001b[39m\u001b[39m'\u001b[39m: ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mFixedLenFeature, ((), ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mstring), \u001b[39mNone\u001b[39;00m, fscope)}\n\u001b[1;32m     11\u001b[0m features \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mparse_single_example, (ag__\u001b[39m.\u001b[39mld(record), ag__\u001b[39m.\u001b[39mld(keys_to_features)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 12\u001b[0m noise_stft_mag_features \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreshape, (ag__\u001b[39m.\u001b[39mld(noise_stft_mag_features), (\u001b[39m1\u001b[39m, ag__\u001b[39m.\u001b[39mld(numSegments), ag__\u001b[39m.\u001b[39mld(numFeatures))), \u001b[39mdict\u001b[39m(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnoise_stft_mag_features\u001b[39m\u001b[39m'\u001b[39m), fscope)\n\u001b[1;32m     13\u001b[0m clean_stft_magnitude \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreshape, (ag__\u001b[39m.\u001b[39mld(clean_stft_magnitude), (\u001b[39m1\u001b[39m, ag__\u001b[39m.\u001b[39mld(numSegments), ag__\u001b[39m.\u001b[39mld(numFeatures))), \u001b[39mdict\u001b[39m(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mclean_stft_magnitude\u001b[39m\u001b[39m'\u001b[39m), fscope)\n\u001b[1;32m     14\u001b[0m noise_stft_phase \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreshape, (ag__\u001b[39m.\u001b[39mld(noise_stft_phase), (ag__\u001b[39m.\u001b[39mld(numFeatures),)), \u001b[39mdict\u001b[39m(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnoise_stft_phase\u001b[39m\u001b[39m'\u001b[39m), fscope)\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: in user code:\n\n    File \"/var/folders/1g/912b41091855kd9w5h7bz27c0000gn/T/ipykernel_10242/2390742533.py\", line 11, in tf_record_parser  *\n        noise_stft_mag_features = tf.reshape(noise_stft_mag_features, (1, numSegments, numFeatures), name=\"noise_stft_mag_features\")\n\n    UnboundLocalError: local variable 'noise_stft_mag_features' referenced before assignment\n"
          ]
        }
      ],
      "source": [
        "train_dataset = tf.data.TFRecordDataset([train_tfrecords_filenames])\n",
        "train_dataset = train_dataset.map(tf_record_parser)\n",
        "train_dataset = train_dataset.shuffle(8192)\n",
        "train_dataset = train_dataset.repeat()\n",
        "train_dataset = train_dataset.batch(512)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOuWPzQaTNDy"
      },
      "outputs": [],
      "source": [
        "test_dataset = tf.data.TFRecordDataset([val_tfrecords_filenames])\n",
        "test_dataset = test_dataset.map(tf_record_parser)\n",
        "test_dataset = test_dataset.repeat(1)\n",
        "test_dataset = test_dataset.batch(512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEG5JofLSfRw"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OZFegXrSYle"
      },
      "outputs": [],
      "source": [
        "from keras.layers import LSTM, Dense, BatchNormalization, Input\n",
        "from keras import Model\n",
        "import keras.regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from librosa.filters import mel\n",
        "\n",
        "def get_mel_filter(samplerate, n_fft, n_mels, fmin, fmax):\n",
        "    mel_basis = mel(sr=samplerate, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
        "    return tf.convert_to_tensor(mel_basis, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model_lstm():\n",
        "  inputs = Input(shape=[1, numSegments, numFeatures])\n",
        "  x = inputs\n",
        "\n",
        "  mask = tf.identity(x)\n",
        "  \n",
        "  mel_matrix = get_mel_filter(samplerate=16000, n_fft=numFeatures, n_mels=numFeatures//2, fmin=0, fmax=8000)\n",
        "  mask = tf.matmul(mask, tf.transpose(mel_matrix, perm=[1, 0]))\n",
        "\n",
        "\n",
        "  mask = LSTM(numFeatures, activation='tanh', return_sequences=True)(mask)\n",
        "  mask = Dense(128, activation='relu', use_bias=True, \n",
        "        kernel_initializer='glorot_uniform', bias_initializer='zeros')(mask)\n",
        "\n",
        "  mask = BatchNormalization()(mask)\n",
        "\n",
        "  mask = LSTM(256, activation='tanh')(mask)\n",
        "  mask = Dense(128, activation='sigmoid', use_bias=True,\n",
        "        kernel_initializer='glorot_uniform', bias_initializer='zeros')(mask)\n",
        "\n",
        "  mask = tf.matmul(mask, mel_matrix)\n",
        "  x = tf.matmul(x, mask)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=x)\n",
        "\n",
        "  optimizer = keras.optimizers.Adam(3e-4)\n",
        "  #optimizer = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=3e-4)\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss='mse', \n",
        "              metrics=[keras.metrics.RootMeanSquaredError('rmse')])\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNpHS4LuShxd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 129, 8, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " zero_padding2d_10 (ZeroPadding  (None, 137, 8, 1)   0           ['input_11[0][0]']               \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " conv2d_145 (Conv2D)            (None, 129, 1, 18)   1296        ['zero_padding2d_10[0][0]']      \n",
            "                                                                                                  \n",
            " activation_135 (Activation)    (None, 129, 1, 18)   0           ['conv2d_145[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_135 (Batch  (None, 129, 1, 18)  72          ['activation_135[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_146 (Conv2D)            (None, 129, 1, 30)   2700        ['batch_normalization_135[0][0]']\n",
            "                                                                                                  \n",
            " activation_136 (Activation)    (None, 129, 1, 30)   0           ['conv2d_146[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_136 (Batch  (None, 129, 1, 30)  120         ['activation_136[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_147 (Conv2D)            (None, 129, 1, 8)    2160        ['batch_normalization_136[0][0]']\n",
            "                                                                                                  \n",
            " activation_137 (Activation)    (None, 129, 1, 8)    0           ['conv2d_147[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_137 (Batch  (None, 129, 1, 8)   32          ['activation_137[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_148 (Conv2D)            (None, 129, 1, 18)   1296        ['batch_normalization_137[0][0]']\n",
            "                                                                                                  \n",
            " activation_138 (Activation)    (None, 129, 1, 18)   0           ['conv2d_148[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_138 (Batch  (None, 129, 1, 18)  72          ['activation_138[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_149 (Conv2D)            (None, 129, 1, 30)   2700        ['batch_normalization_138[0][0]']\n",
            "                                                                                                  \n",
            " activation_139 (Activation)    (None, 129, 1, 30)   0           ['conv2d_149[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_139 (Batch  (None, 129, 1, 30)  120         ['activation_139[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_150 (Conv2D)            (None, 129, 1, 8)    2160        ['batch_normalization_139[0][0]']\n",
            "                                                                                                  \n",
            " activation_140 (Activation)    (None, 129, 1, 8)    0           ['conv2d_150[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_140 (Batch  (None, 129, 1, 8)   32          ['activation_140[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_151 (Conv2D)            (None, 129, 1, 18)   1296        ['batch_normalization_140[0][0]']\n",
            "                                                                                                  \n",
            " activation_141 (Activation)    (None, 129, 1, 18)   0           ['conv2d_151[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_141 (Batch  (None, 129, 1, 18)  72          ['activation_141[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_152 (Conv2D)            (None, 129, 1, 30)   2700        ['batch_normalization_141[0][0]']\n",
            "                                                                                                  \n",
            " activation_142 (Activation)    (None, 129, 1, 30)   0           ['conv2d_152[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_142 (Batch  (None, 129, 1, 30)  120         ['activation_142[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_153 (Conv2D)            (None, 129, 1, 8)    2160        ['batch_normalization_142[0][0]']\n",
            "                                                                                                  \n",
            " activation_143 (Activation)    (None, 129, 1, 8)    0           ['conv2d_153[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_143 (Batch  (None, 129, 1, 8)   32          ['activation_143[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_154 (Conv2D)            (None, 129, 1, 18)   1296        ['batch_normalization_143[0][0]']\n",
            "                                                                                                  \n",
            " activation_144 (Activation)    (None, 129, 1, 18)   0           ['conv2d_154[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_144 (Batch  (None, 129, 1, 18)  72          ['activation_144[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_155 (Conv2D)            (None, 129, 1, 30)   2700        ['batch_normalization_144[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_18 (TFOpL  (None, 129, 1, 30)  0           ['conv2d_155[0][0]',             \n",
            " ambda)                                                           'conv2d_149[0][0]']             \n",
            "                                                                                                  \n",
            " activation_145 (Activation)    (None, 129, 1, 30)   0           ['tf.__operators__.add_18[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_145 (Batch  (None, 129, 1, 30)  120         ['activation_145[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_156 (Conv2D)            (None, 129, 1, 8)    2160        ['batch_normalization_145[0][0]']\n",
            "                                                                                                  \n",
            " activation_146 (Activation)    (None, 129, 1, 8)    0           ['conv2d_156[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_146 (Batch  (None, 129, 1, 8)   32          ['activation_146[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_157 (Conv2D)            (None, 129, 1, 18)   1296        ['batch_normalization_146[0][0]']\n",
            "                                                                                                  \n",
            " activation_147 (Activation)    (None, 129, 1, 18)   0           ['conv2d_157[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_147 (Batch  (None, 129, 1, 18)  72          ['activation_147[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_158 (Conv2D)            (None, 129, 1, 30)   2700        ['batch_normalization_147[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_19 (TFOpL  (None, 129, 1, 30)  0           ['conv2d_158[0][0]',             \n",
            " ambda)                                                           'conv2d_146[0][0]']             \n",
            "                                                                                                  \n",
            " activation_148 (Activation)    (None, 129, 1, 30)   0           ['tf.__operators__.add_19[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_148 (Batch  (None, 129, 1, 30)  120         ['activation_148[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_159 (Conv2D)            (None, 129, 1, 8)    2160        ['batch_normalization_148[0][0]']\n",
            "                                                                                                  \n",
            " activation_149 (Activation)    (None, 129, 1, 8)    0           ['conv2d_159[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_149 (Batch  (None, 129, 1, 8)   32          ['activation_149[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " spatial_dropout2d_9 (SpatialDr  (None, 129, 1, 8)   0           ['batch_normalization_149[0][0]']\n",
            " opout2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_160 (Conv2D)            (None, 129, 1, 1)    1033        ['spatial_dropout2d_9[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 32,933\n",
            "Trainable params: 32,373\n",
            "Non-trainable params: 560\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_model_lstm()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rWOgsdwglFE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
          ]
        }
      ],
      "source": [
        "# You might need to install the following dependencies: sudo apt install python-pydot python-pydot-ng graphviz\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, dpi=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OdmgHTp4_Ou"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6007 (pid 498806), started 1:31:32 ago. (Use '!kill 498806' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-c62ef9c2cda22c7e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-c62ef9c2cda22c7e\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6007;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "BucSAwkHQj8Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4081/4081 [==============================] - 25s 6ms/step - loss: 0.6973 - rmse: 0.8351\n",
            "Baseline accuracy 0.6973141431808472\n"
          ]
        }
      ],
      "source": [
        "baseline_val_loss = model.evaluate(test_dataset)[0]\n",
        "print(f\"Baseline accuracy {baseline_val_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "ocycFbP5-X0o"
      },
      "outputs": [],
      "source": [
        "def l2_norm(vector):\n",
        "    return np.square(vector)\n",
        "\n",
        "def SDR(denoised, cleaned, eps=1e-7): # Signal to Distortion Ratio\n",
        "    a = l2_norm(denoised)\n",
        "    b = l2_norm(denoised - cleaned)\n",
        "    a_b = a / b\n",
        "    return np.mean(10 * np.log10(a_b + eps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "bcPAuMZ9SlHa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "600/600 [==============================] - 31s 49ms/step - loss: 0.4195 - rmse: 0.6477 - val_loss: 0.2765 - val_rmse: 0.5259\n",
            "Epoch 2/400\n",
            "600/600 [==============================] - 31s 52ms/step - loss: 0.2632 - rmse: 0.5130 - val_loss: 0.2316 - val_rmse: 0.4812\n",
            "Epoch 3/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.2465 - rmse: 0.4965 - val_loss: 0.2232 - val_rmse: 0.4725\n",
            "Epoch 4/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.2272 - rmse: 0.4767 - val_loss: 0.2348 - val_rmse: 0.4846\n",
            "Epoch 5/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.2335 - rmse: 0.4832 - val_loss: 0.2082 - val_rmse: 0.4562\n",
            "Epoch 6/400\n",
            "600/600 [==============================] - 29s 49ms/step - loss: 0.2095 - rmse: 0.4577 - val_loss: 0.2024 - val_rmse: 0.4499\n",
            "Epoch 7/400\n",
            "600/600 [==============================] - 28s 48ms/step - loss: 0.2118 - rmse: 0.4602 - val_loss: 0.2078 - val_rmse: 0.4558\n",
            "Epoch 8/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.2061 - rmse: 0.4539 - val_loss: 0.2057 - val_rmse: 0.4535\n",
            "Epoch 9/400\n",
            "600/600 [==============================] - 30s 49ms/step - loss: 0.2024 - rmse: 0.4499 - val_loss: 0.1951 - val_rmse: 0.4417\n",
            "Epoch 10/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.2036 - rmse: 0.4512 - val_loss: 0.2157 - val_rmse: 0.4644\n",
            "Epoch 11/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1964 - rmse: 0.4432 - val_loss: 0.2079 - val_rmse: 0.4560\n",
            "Epoch 12/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1987 - rmse: 0.4458 - val_loss: 0.1949 - val_rmse: 0.4415\n",
            "Epoch 13/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1989 - rmse: 0.4460 - val_loss: 0.1968 - val_rmse: 0.4436\n",
            "Epoch 14/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.2057 - rmse: 0.4536 - val_loss: 0.2125 - val_rmse: 0.4610\n",
            "Epoch 15/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.2020 - rmse: 0.4495 - val_loss: 0.1887 - val_rmse: 0.4344\n",
            "Epoch 16/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1885 - rmse: 0.4342 - val_loss: 0.1991 - val_rmse: 0.4462\n",
            "Epoch 17/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1940 - rmse: 0.4404 - val_loss: 0.2103 - val_rmse: 0.4586\n",
            "Epoch 18/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1948 - rmse: 0.4414 - val_loss: 0.1958 - val_rmse: 0.4425\n",
            "Epoch 19/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1978 - rmse: 0.4448 - val_loss: 0.1973 - val_rmse: 0.4442\n",
            "Epoch 20/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1957 - rmse: 0.4424 - val_loss: 0.1891 - val_rmse: 0.4349\n",
            "Epoch 21/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.2056 - rmse: 0.4535 - val_loss: 0.2135 - val_rmse: 0.4620\n",
            "Epoch 22/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1854 - rmse: 0.4305 - val_loss: 0.1874 - val_rmse: 0.4329\n",
            "Epoch 23/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1919 - rmse: 0.4381 - val_loss: 0.1819 - val_rmse: 0.4265\n",
            "Epoch 24/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1873 - rmse: 0.4328 - val_loss: 0.1929 - val_rmse: 0.4392\n",
            "Epoch 25/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1861 - rmse: 0.4313 - val_loss: 0.1897 - val_rmse: 0.4355\n",
            "Epoch 26/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1891 - rmse: 0.4348 - val_loss: 0.1883 - val_rmse: 0.4340\n",
            "Epoch 27/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1804 - rmse: 0.4247 - val_loss: 0.1813 - val_rmse: 0.4258\n",
            "Epoch 28/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1873 - rmse: 0.4328 - val_loss: 0.2214 - val_rmse: 0.4705\n",
            "Epoch 29/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1879 - rmse: 0.4334 - val_loss: 0.2016 - val_rmse: 0.4490\n",
            "Epoch 30/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1946 - rmse: 0.4412 - val_loss: 0.1909 - val_rmse: 0.4369\n",
            "Epoch 31/400\n",
            "600/600 [==============================] - 31s 52ms/step - loss: 0.1898 - rmse: 0.4357 - val_loss: 0.1793 - val_rmse: 0.4235\n",
            "Epoch 32/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1821 - rmse: 0.4268 - val_loss: 0.1815 - val_rmse: 0.4260\n",
            "Epoch 33/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1852 - rmse: 0.4304 - val_loss: 0.1814 - val_rmse: 0.4259\n",
            "Epoch 34/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1892 - rmse: 0.4349 - val_loss: 0.2085 - val_rmse: 0.4566\n",
            "Epoch 35/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1924 - rmse: 0.4386 - val_loss: 0.1849 - val_rmse: 0.4300\n",
            "Epoch 36/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1888 - rmse: 0.4345 - val_loss: 0.1857 - val_rmse: 0.4310\n",
            "Epoch 37/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1987 - rmse: 0.4457 - val_loss: 0.1826 - val_rmse: 0.4273\n",
            "Epoch 38/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1803 - rmse: 0.4246 - val_loss: 0.1815 - val_rmse: 0.4260\n",
            "Epoch 39/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1841 - rmse: 0.4291 - val_loss: 0.1772 - val_rmse: 0.4209\n",
            "Epoch 40/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1841 - rmse: 0.4290 - val_loss: 0.1747 - val_rmse: 0.4180\n",
            "Epoch 41/400\n",
            "600/600 [==============================] - 29s 49ms/step - loss: 0.1845 - rmse: 0.4296 - val_loss: 0.1819 - val_rmse: 0.4264\n",
            "Epoch 42/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1809 - rmse: 0.4253 - val_loss: 0.1852 - val_rmse: 0.4304\n",
            "Epoch 43/400\n",
            "600/600 [==============================] - 29s 49ms/step - loss: 0.1787 - rmse: 0.4227 - val_loss: 0.1777 - val_rmse: 0.4215\n",
            "Epoch 44/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1820 - rmse: 0.4266 - val_loss: 0.1775 - val_rmse: 0.4213\n",
            "Epoch 45/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1842 - rmse: 0.4292 - val_loss: 0.2032 - val_rmse: 0.4508\n",
            "Epoch 46/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1901 - rmse: 0.4360 - val_loss: 0.1787 - val_rmse: 0.4227\n",
            "Epoch 47/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1851 - rmse: 0.4302 - val_loss: 0.1806 - val_rmse: 0.4250\n",
            "Epoch 48/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1790 - rmse: 0.4230 - val_loss: 0.2042 - val_rmse: 0.4518\n",
            "Epoch 49/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1863 - rmse: 0.4316 - val_loss: 0.1868 - val_rmse: 0.4322\n",
            "Epoch 50/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1826 - rmse: 0.4274 - val_loss: 0.1793 - val_rmse: 0.4234\n",
            "Epoch 51/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1886 - rmse: 0.4342 - val_loss: 0.1900 - val_rmse: 0.4359\n",
            "Epoch 52/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1876 - rmse: 0.4332 - val_loss: 0.1844 - val_rmse: 0.4294\n",
            "Epoch 53/400\n",
            "600/600 [==============================] - 25s 43ms/step - loss: 0.1901 - rmse: 0.4360 - val_loss: 0.2030 - val_rmse: 0.4506\n",
            "Epoch 54/400\n",
            "600/600 [==============================] - 24s 41ms/step - loss: 0.1797 - rmse: 0.4240 - val_loss: 0.1750 - val_rmse: 0.4184\n",
            "Epoch 55/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1796 - rmse: 0.4238 - val_loss: 0.1857 - val_rmse: 0.4310\n",
            "Epoch 56/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1835 - rmse: 0.4283 - val_loss: 0.1720 - val_rmse: 0.4147\n",
            "Epoch 57/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1805 - rmse: 0.4248 - val_loss: 0.1835 - val_rmse: 0.4283\n",
            "Epoch 58/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1768 - rmse: 0.4205 - val_loss: 0.1739 - val_rmse: 0.4170\n",
            "Epoch 59/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1787 - rmse: 0.4228 - val_loss: 0.1852 - val_rmse: 0.4304\n",
            "Epoch 60/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1793 - rmse: 0.4235 - val_loss: 0.1775 - val_rmse: 0.4213\n",
            "Epoch 61/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1824 - rmse: 0.4271 - val_loss: 0.1913 - val_rmse: 0.4374\n",
            "Epoch 62/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1858 - rmse: 0.4311 - val_loss: 0.1770 - val_rmse: 0.4208\n",
            "Epoch 63/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1844 - rmse: 0.4294 - val_loss: 0.1811 - val_rmse: 0.4256\n",
            "Epoch 64/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1756 - rmse: 0.4190 - val_loss: 0.1743 - val_rmse: 0.4175\n",
            "Epoch 65/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1840 - rmse: 0.4290 - val_loss: 0.1741 - val_rmse: 0.4172\n",
            "Epoch 66/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1850 - rmse: 0.4301 - val_loss: 0.1745 - val_rmse: 0.4177\n",
            "Epoch 67/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1829 - rmse: 0.4276 - val_loss: 0.1746 - val_rmse: 0.4179\n",
            "Epoch 68/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1886 - rmse: 0.4343 - val_loss: 0.1747 - val_rmse: 0.4180\n",
            "Epoch 69/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1853 - rmse: 0.4304 - val_loss: 0.1737 - val_rmse: 0.4168\n",
            "Epoch 70/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1782 - rmse: 0.4222 - val_loss: 0.1705 - val_rmse: 0.4129\n",
            "Epoch 71/400\n",
            "600/600 [==============================] - 30s 51ms/step - loss: 0.1774 - rmse: 0.4212 - val_loss: 0.1909 - val_rmse: 0.4369\n",
            "Epoch 72/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1806 - rmse: 0.4250 - val_loss: 0.2078 - val_rmse: 0.4559\n",
            "Epoch 73/400\n",
            "600/600 [==============================] - 29s 49ms/step - loss: 0.1795 - rmse: 0.4237 - val_loss: 0.1724 - val_rmse: 0.4152\n",
            "Epoch 74/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1761 - rmse: 0.4196 - val_loss: 0.1741 - val_rmse: 0.4173\n",
            "Epoch 75/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1749 - rmse: 0.4182 - val_loss: 0.1806 - val_rmse: 0.4250\n",
            "Epoch 76/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1765 - rmse: 0.4201 - val_loss: 0.1736 - val_rmse: 0.4166\n",
            "Epoch 77/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1829 - rmse: 0.4276 - val_loss: 0.1772 - val_rmse: 0.4209\n",
            "Epoch 78/400\n",
            "600/600 [==============================] - 30s 51ms/step - loss: 0.1847 - rmse: 0.4297 - val_loss: 0.1724 - val_rmse: 0.4152\n",
            "Epoch 79/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1796 - rmse: 0.4238 - val_loss: 0.1739 - val_rmse: 0.4170\n",
            "Epoch 80/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1754 - rmse: 0.4188 - val_loss: 0.1711 - val_rmse: 0.4136\n",
            "Epoch 81/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1824 - rmse: 0.4271 - val_loss: 0.1844 - val_rmse: 0.4294\n",
            "Epoch 82/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1875 - rmse: 0.4330 - val_loss: 0.1742 - val_rmse: 0.4173\n",
            "Epoch 83/400\n",
            "600/600 [==============================] - 31s 51ms/step - loss: 0.1777 - rmse: 0.4215 - val_loss: 0.1844 - val_rmse: 0.4294\n",
            "Epoch 84/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1869 - rmse: 0.4323 - val_loss: 0.1717 - val_rmse: 0.4143\n",
            "Epoch 85/400\n",
            "600/600 [==============================] - 30s 51ms/step - loss: 0.1827 - rmse: 0.4275 - val_loss: 0.1713 - val_rmse: 0.4139\n",
            "Epoch 86/400\n",
            "600/600 [==============================] - 30s 50ms/step - loss: 0.1783 - rmse: 0.4222 - val_loss: 0.1850 - val_rmse: 0.4301\n",
            "Epoch 87/400\n",
            "600/600 [==============================] - 29s 49ms/step - loss: 0.1746 - rmse: 0.4178 - val_loss: 0.1703 - val_rmse: 0.4126\n",
            "Epoch 88/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1772 - rmse: 0.4210 - val_loss: 0.1797 - val_rmse: 0.4239\n",
            "Epoch 89/400\n",
            "600/600 [==============================] - 29s 49ms/step - loss: 0.1767 - rmse: 0.4204 - val_loss: 0.1714 - val_rmse: 0.4140\n",
            "Epoch 90/400\n",
            "600/600 [==============================] - 31s 52ms/step - loss: 0.1739 - rmse: 0.4170 - val_loss: 0.1789 - val_rmse: 0.4230\n",
            "Epoch 91/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1752 - rmse: 0.4185 - val_loss: 0.1823 - val_rmse: 0.4270\n",
            "Epoch 92/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1748 - rmse: 0.4181 - val_loss: 0.1741 - val_rmse: 0.4173\n",
            "Epoch 93/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1811 - rmse: 0.4256 - val_loss: 0.1742 - val_rmse: 0.4173\n",
            "Epoch 94/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1849 - rmse: 0.4300 - val_loss: 0.1759 - val_rmse: 0.4194\n",
            "Epoch 95/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1756 - rmse: 0.4190 - val_loss: 0.1723 - val_rmse: 0.4151\n",
            "Epoch 96/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1748 - rmse: 0.4181 - val_loss: 0.1865 - val_rmse: 0.4318\n",
            "Epoch 97/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1816 - rmse: 0.4262 - val_loss: 0.1711 - val_rmse: 0.4136\n",
            "Epoch 98/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1859 - rmse: 0.4312 - val_loss: 0.1771 - val_rmse: 0.4209\n",
            "Epoch 99/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1778 - rmse: 0.4217 - val_loss: 0.1785 - val_rmse: 0.4225\n",
            "Epoch 100/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1890 - rmse: 0.4348 - val_loss: 0.1721 - val_rmse: 0.4149\n",
            "Epoch 101/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1760 - rmse: 0.4195 - val_loss: 0.1699 - val_rmse: 0.4122\n",
            "Epoch 102/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1775 - rmse: 0.4213 - val_loss: 0.1819 - val_rmse: 0.4265\n",
            "Epoch 103/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1738 - rmse: 0.4170 - val_loss: 0.1717 - val_rmse: 0.4143\n",
            "Epoch 104/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1755 - rmse: 0.4190 - val_loss: 0.1770 - val_rmse: 0.4207\n",
            "Epoch 105/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1757 - rmse: 0.4191 - val_loss: 0.1738 - val_rmse: 0.4168\n",
            "Epoch 106/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1720 - rmse: 0.4147 - val_loss: 0.1763 - val_rmse: 0.4198\n",
            "Epoch 107/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1745 - rmse: 0.4177 - val_loss: 0.1720 - val_rmse: 0.4147\n",
            "Epoch 108/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1765 - rmse: 0.4201 - val_loss: 0.2044 - val_rmse: 0.4522\n",
            "Epoch 109/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1819 - rmse: 0.4265 - val_loss: 0.1684 - val_rmse: 0.4103\n",
            "Epoch 110/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1824 - rmse: 0.4271 - val_loss: 0.1862 - val_rmse: 0.4315\n",
            "Epoch 111/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1727 - rmse: 0.4156 - val_loss: 0.1776 - val_rmse: 0.4215\n",
            "Epoch 112/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1748 - rmse: 0.4181 - val_loss: 0.1694 - val_rmse: 0.4115\n",
            "Epoch 113/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1784 - rmse: 0.4224 - val_loss: 0.1691 - val_rmse: 0.4112\n",
            "Epoch 114/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1860 - rmse: 0.4313 - val_loss: 0.1684 - val_rmse: 0.4103\n",
            "Epoch 115/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1777 - rmse: 0.4215 - val_loss: 0.1728 - val_rmse: 0.4157\n",
            "Epoch 116/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1883 - rmse: 0.4339 - val_loss: 0.1671 - val_rmse: 0.4088\n",
            "Epoch 117/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1742 - rmse: 0.4173 - val_loss: 0.1970 - val_rmse: 0.4439\n",
            "Epoch 118/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1766 - rmse: 0.4202 - val_loss: 0.2007 - val_rmse: 0.4480\n",
            "Epoch 119/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1731 - rmse: 0.4161 - val_loss: 0.1741 - val_rmse: 0.4172\n",
            "Epoch 120/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1739 - rmse: 0.4170 - val_loss: 0.1676 - val_rmse: 0.4094\n",
            "Epoch 121/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1759 - rmse: 0.4195 - val_loss: 0.1780 - val_rmse: 0.4218\n",
            "Epoch 122/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1702 - rmse: 0.4126 - val_loss: 0.1731 - val_rmse: 0.4161\n",
            "Epoch 123/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1731 - rmse: 0.4161 - val_loss: 0.1760 - val_rmse: 0.4195\n",
            "Epoch 124/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1758 - rmse: 0.4192 - val_loss: 0.1724 - val_rmse: 0.4152\n",
            "Epoch 125/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1823 - rmse: 0.4269 - val_loss: 0.1773 - val_rmse: 0.4211\n",
            "Epoch 126/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1834 - rmse: 0.4283 - val_loss: 0.1696 - val_rmse: 0.4118\n",
            "Epoch 127/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1681 - rmse: 0.4100 - val_loss: 0.1769 - val_rmse: 0.4206\n",
            "Epoch 128/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1762 - rmse: 0.4198 - val_loss: 0.1680 - val_rmse: 0.4098\n",
            "Epoch 129/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1763 - rmse: 0.4198 - val_loss: 0.1692 - val_rmse: 0.4113\n",
            "Epoch 130/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1824 - rmse: 0.4271 - val_loss: 0.1749 - val_rmse: 0.4182\n",
            "Epoch 131/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1804 - rmse: 0.4248 - val_loss: 0.1693 - val_rmse: 0.4114\n",
            "Epoch 132/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1882 - rmse: 0.4338 - val_loss: 0.1676 - val_rmse: 0.4094\n",
            "Epoch 133/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1709 - rmse: 0.4134 - val_loss: 0.1817 - val_rmse: 0.4262\n",
            "Epoch 134/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1776 - rmse: 0.4214 - val_loss: 0.1763 - val_rmse: 0.4199\n",
            "Epoch 135/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1721 - rmse: 0.4148 - val_loss: 0.1709 - val_rmse: 0.4135\n",
            "Epoch 136/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1733 - rmse: 0.4163 - val_loss: 0.1832 - val_rmse: 0.4281\n",
            "Epoch 137/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1746 - rmse: 0.4179 - val_loss: 0.1718 - val_rmse: 0.4145\n",
            "Epoch 138/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1685 - rmse: 0.4105 - val_loss: 0.1683 - val_rmse: 0.4102\n",
            "Epoch 139/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1746 - rmse: 0.4179 - val_loss: 0.1685 - val_rmse: 0.4105\n",
            "Epoch 140/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1739 - rmse: 0.4171 - val_loss: 0.1689 - val_rmse: 0.4110\n",
            "Epoch 141/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1816 - rmse: 0.4261 - val_loss: 0.1762 - val_rmse: 0.4198\n",
            "Epoch 142/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1804 - rmse: 0.4247 - val_loss: 0.1716 - val_rmse: 0.4142\n",
            "Epoch 143/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1681 - rmse: 0.4100 - val_loss: 0.1656 - val_rmse: 0.4069\n",
            "Epoch 144/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1751 - rmse: 0.4185 - val_loss: 0.1857 - val_rmse: 0.4310\n",
            "Epoch 145/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1766 - rmse: 0.4202 - val_loss: 0.1684 - val_rmse: 0.4104\n",
            "Epoch 146/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1802 - rmse: 0.4245 - val_loss: 0.1659 - val_rmse: 0.4073\n",
            "Epoch 147/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1785 - rmse: 0.4225 - val_loss: 0.1719 - val_rmse: 0.4146\n",
            "Epoch 148/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1895 - rmse: 0.4353 - val_loss: 0.1693 - val_rmse: 0.4115\n",
            "Epoch 149/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1690 - rmse: 0.4111 - val_loss: 0.1691 - val_rmse: 0.4112\n",
            "Epoch 150/400\n",
            "600/600 [==============================] - 25s 43ms/step - loss: 0.1760 - rmse: 0.4195 - val_loss: 0.1797 - val_rmse: 0.4239\n",
            "Epoch 151/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1725 - rmse: 0.4153 - val_loss: 0.1707 - val_rmse: 0.4132\n",
            "Epoch 152/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1724 - rmse: 0.4152 - val_loss: 0.1783 - val_rmse: 0.4223\n",
            "Epoch 153/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1751 - rmse: 0.4184 - val_loss: 0.1693 - val_rmse: 0.4115\n",
            "Epoch 154/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1667 - rmse: 0.4083 - val_loss: 0.1707 - val_rmse: 0.4132\n",
            "Epoch 155/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1730 - rmse: 0.4159 - val_loss: 0.1793 - val_rmse: 0.4234\n",
            "Epoch 156/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1747 - rmse: 0.4179 - val_loss: 0.1705 - val_rmse: 0.4129\n",
            "Epoch 157/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1811 - rmse: 0.4255 - val_loss: 0.1711 - val_rmse: 0.4136\n",
            "Epoch 158/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1776 - rmse: 0.4214 - val_loss: 0.1708 - val_rmse: 0.4133\n",
            "Epoch 159/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1699 - rmse: 0.4122 - val_loss: 0.1666 - val_rmse: 0.4081\n",
            "Epoch 160/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1768 - rmse: 0.4204 - val_loss: 0.1738 - val_rmse: 0.4168\n",
            "Epoch 161/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1733 - rmse: 0.4163 - val_loss: 0.1663 - val_rmse: 0.4078\n",
            "Epoch 162/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1803 - rmse: 0.4247 - val_loss: 0.1647 - val_rmse: 0.4058\n",
            "Epoch 163/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1790 - rmse: 0.4231 - val_loss: 0.1692 - val_rmse: 0.4114\n",
            "Epoch 164/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1845 - rmse: 0.4295 - val_loss: 0.1690 - val_rmse: 0.4111\n",
            "Epoch 165/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1714 - rmse: 0.4140 - val_loss: 0.1831 - val_rmse: 0.4279\n",
            "Epoch 166/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1721 - rmse: 0.4148 - val_loss: 0.1664 - val_rmse: 0.4079\n",
            "Epoch 167/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1744 - rmse: 0.4176 - val_loss: 0.1674 - val_rmse: 0.4092\n",
            "Epoch 168/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1739 - rmse: 0.4170 - val_loss: 0.1679 - val_rmse: 0.4097\n",
            "Epoch 169/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1696 - rmse: 0.4118 - val_loss: 0.1908 - val_rmse: 0.4369\n",
            "Epoch 170/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1696 - rmse: 0.4118 - val_loss: 0.1691 - val_rmse: 0.4113\n",
            "Epoch 171/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1728 - rmse: 0.4157 - val_loss: 0.1711 - val_rmse: 0.4137\n",
            "Epoch 172/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1741 - rmse: 0.4172 - val_loss: 0.1673 - val_rmse: 0.4090\n",
            "Epoch 173/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1791 - rmse: 0.4232 - val_loss: 0.1729 - val_rmse: 0.4158\n",
            "Epoch 174/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1765 - rmse: 0.4201 - val_loss: 0.1634 - val_rmse: 0.4042\n",
            "Epoch 175/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1694 - rmse: 0.4116 - val_loss: 0.1675 - val_rmse: 0.4092\n",
            "Epoch 176/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1769 - rmse: 0.4206 - val_loss: 0.1894 - val_rmse: 0.4352\n",
            "Epoch 177/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1745 - rmse: 0.4178 - val_loss: 0.1700 - val_rmse: 0.4123\n",
            "Epoch 178/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1790 - rmse: 0.4231 - val_loss: 0.1666 - val_rmse: 0.4082\n",
            "Epoch 179/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1794 - rmse: 0.4236 - val_loss: 0.1759 - val_rmse: 0.4195\n",
            "Epoch 180/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1810 - rmse: 0.4255 - val_loss: 0.1795 - val_rmse: 0.4237\n",
            "Epoch 181/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1715 - rmse: 0.4141 - val_loss: 0.1728 - val_rmse: 0.4157\n",
            "Epoch 182/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1717 - rmse: 0.4144 - val_loss: 0.1843 - val_rmse: 0.4293\n",
            "Epoch 183/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1744 - rmse: 0.4177 - val_loss: 0.1656 - val_rmse: 0.4069\n",
            "Epoch 184/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1737 - rmse: 0.4167 - val_loss: 0.2153 - val_rmse: 0.4640\n",
            "Epoch 185/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1694 - rmse: 0.4116 - val_loss: 0.1682 - val_rmse: 0.4101\n",
            "Epoch 186/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1688 - rmse: 0.4108 - val_loss: 0.1676 - val_rmse: 0.4093\n",
            "Epoch 187/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1713 - rmse: 0.4139 - val_loss: 0.1656 - val_rmse: 0.4069\n",
            "Epoch 188/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1747 - rmse: 0.4180 - val_loss: 0.1653 - val_rmse: 0.4065\n",
            "Epoch 189/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1773 - rmse: 0.4211 - val_loss: 0.1651 - val_rmse: 0.4063\n",
            "Epoch 190/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1748 - rmse: 0.4181 - val_loss: 0.1747 - val_rmse: 0.4180\n",
            "Epoch 191/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1684 - rmse: 0.4103 - val_loss: 0.1762 - val_rmse: 0.4198\n",
            "Epoch 192/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1770 - rmse: 0.4208 - val_loss: 0.1730 - val_rmse: 0.4159\n",
            "Epoch 193/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1788 - rmse: 0.4229 - val_loss: 0.1663 - val_rmse: 0.4078\n",
            "Epoch 194/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1748 - rmse: 0.4181 - val_loss: 0.1715 - val_rmse: 0.4142\n",
            "Epoch 195/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1814 - rmse: 0.4259 - val_loss: 0.1690 - val_rmse: 0.4111\n",
            "Epoch 196/400\n",
            "600/600 [==============================] - 25s 43ms/step - loss: 0.1792 - rmse: 0.4233 - val_loss: 0.1805 - val_rmse: 0.4249\n",
            "Epoch 197/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1708 - rmse: 0.4133 - val_loss: 0.1645 - val_rmse: 0.4056\n",
            "Epoch 198/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1710 - rmse: 0.4135 - val_loss: 0.1789 - val_rmse: 0.4229\n",
            "Epoch 199/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1719 - rmse: 0.4146 - val_loss: 0.1636 - val_rmse: 0.4045\n",
            "Epoch 200/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1721 - rmse: 0.4148 - val_loss: 0.1686 - val_rmse: 0.4106\n",
            "Epoch 201/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1703 - rmse: 0.4126 - val_loss: 0.1708 - val_rmse: 0.4133\n",
            "Epoch 202/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1688 - rmse: 0.4108 - val_loss: 0.1744 - val_rmse: 0.4176\n",
            "Epoch 203/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1684 - rmse: 0.4104 - val_loss: 0.1701 - val_rmse: 0.4124\n",
            "Epoch 204/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1764 - rmse: 0.4200 - val_loss: 0.1670 - val_rmse: 0.4086\n",
            "Epoch 205/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1788 - rmse: 0.4228 - val_loss: 0.1862 - val_rmse: 0.4315\n",
            "Epoch 206/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1732 - rmse: 0.4161 - val_loss: 0.1659 - val_rmse: 0.4073\n",
            "Epoch 207/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1694 - rmse: 0.4116 - val_loss: 0.1673 - val_rmse: 0.4090\n",
            "Epoch 208/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1763 - rmse: 0.4199 - val_loss: 0.1774 - val_rmse: 0.4212\n",
            "Epoch 209/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1817 - rmse: 0.4262 - val_loss: 0.1653 - val_rmse: 0.4065\n",
            "Epoch 210/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1705 - rmse: 0.4129 - val_loss: 0.1756 - val_rmse: 0.4191\n",
            "Epoch 211/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1819 - rmse: 0.4265 - val_loss: 0.1890 - val_rmse: 0.4347\n",
            "Epoch 212/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1757 - rmse: 0.4191 - val_loss: 0.1657 - val_rmse: 0.4071\n",
            "Epoch 213/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1730 - rmse: 0.4160 - val_loss: 0.1761 - val_rmse: 0.4196\n",
            "Epoch 214/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1697 - rmse: 0.4120 - val_loss: 0.1666 - val_rmse: 0.4082\n",
            "Epoch 215/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1704 - rmse: 0.4128 - val_loss: 0.1629 - val_rmse: 0.4036\n",
            "Epoch 216/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1713 - rmse: 0.4139 - val_loss: 0.1673 - val_rmse: 0.4090\n",
            "Epoch 217/400\n",
            "600/600 [==============================] - 29s 48ms/step - loss: 0.1679 - rmse: 0.4097 - val_loss: 0.1686 - val_rmse: 0.4106\n",
            "Epoch 218/400\n",
            "600/600 [==============================] - 30s 49ms/step - loss: 0.1697 - rmse: 0.4119 - val_loss: 0.1748 - val_rmse: 0.4181\n",
            "Epoch 219/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1696 - rmse: 0.4118 - val_loss: 0.1686 - val_rmse: 0.4106\n",
            "Epoch 220/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1765 - rmse: 0.4202 - val_loss: 0.1656 - val_rmse: 0.4069\n",
            "Epoch 221/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1791 - rmse: 0.4232 - val_loss: 0.1684 - val_rmse: 0.4104\n",
            "Epoch 222/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1696 - rmse: 0.4118 - val_loss: 0.1662 - val_rmse: 0.4077\n",
            "Epoch 223/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1704 - rmse: 0.4128 - val_loss: 0.1644 - val_rmse: 0.4055\n",
            "Epoch 224/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1756 - rmse: 0.4190 - val_loss: 0.1627 - val_rmse: 0.4034\n",
            "Epoch 225/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1805 - rmse: 0.4248 - val_loss: 0.1846 - val_rmse: 0.4296\n",
            "Epoch 226/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1730 - rmse: 0.4159 - val_loss: 0.1647 - val_rmse: 0.4059\n",
            "Epoch 227/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1855 - rmse: 0.4307 - val_loss: 0.1827 - val_rmse: 0.4274\n",
            "Epoch 228/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1689 - rmse: 0.4109 - val_loss: 0.1753 - val_rmse: 0.4187\n",
            "Epoch 229/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1723 - rmse: 0.4151 - val_loss: 0.1702 - val_rmse: 0.4125\n",
            "Epoch 230/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1690 - rmse: 0.4111 - val_loss: 0.1753 - val_rmse: 0.4187\n",
            "Epoch 231/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1701 - rmse: 0.4124 - val_loss: 0.1619 - val_rmse: 0.4023\n",
            "Epoch 232/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1723 - rmse: 0.4151 - val_loss: 0.1659 - val_rmse: 0.4073\n",
            "Epoch 233/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1653 - rmse: 0.4066 - val_loss: 0.1625 - val_rmse: 0.4031\n",
            "Epoch 234/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1700 - rmse: 0.4123 - val_loss: 0.1637 - val_rmse: 0.4046\n",
            "Epoch 235/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1716 - rmse: 0.4142 - val_loss: 0.1671 - val_rmse: 0.4088\n",
            "Epoch 236/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1763 - rmse: 0.4198 - val_loss: 0.1679 - val_rmse: 0.4097\n",
            "Epoch 237/400\n",
            "600/600 [==============================] - 24s 40ms/step - loss: 0.1787 - rmse: 0.4227 - val_loss: 0.1650 - val_rmse: 0.4062\n",
            "Epoch 238/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1677 - rmse: 0.4095 - val_loss: 0.1670 - val_rmse: 0.4086\n",
            "Epoch 239/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1699 - rmse: 0.4122 - val_loss: 0.1768 - val_rmse: 0.4205\n",
            "Epoch 240/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1737 - rmse: 0.4168 - val_loss: 0.1683 - val_rmse: 0.4103\n",
            "Epoch 241/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1799 - rmse: 0.4242 - val_loss: 0.1731 - val_rmse: 0.4161\n",
            "Epoch 242/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1729 - rmse: 0.4158 - val_loss: 0.1694 - val_rmse: 0.4116\n",
            "Epoch 243/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1857 - rmse: 0.4309 - val_loss: 0.1659 - val_rmse: 0.4074\n",
            "Epoch 244/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1682 - rmse: 0.4101 - val_loss: 0.1659 - val_rmse: 0.4073\n",
            "Epoch 245/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1732 - rmse: 0.4161 - val_loss: 0.1637 - val_rmse: 0.4046\n",
            "Epoch 246/400\n",
            "600/600 [==============================] - 28s 47ms/step - loss: 0.1690 - rmse: 0.4111 - val_loss: 0.1628 - val_rmse: 0.4035\n",
            "Epoch 247/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1697 - rmse: 0.4119 - val_loss: 0.1746 - val_rmse: 0.4178\n",
            "Epoch 248/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1705 - rmse: 0.4129 - val_loss: 0.1671 - val_rmse: 0.4087\n",
            "Epoch 249/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1655 - rmse: 0.4068 - val_loss: 0.1718 - val_rmse: 0.4144\n",
            "Epoch 250/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1689 - rmse: 0.4109 - val_loss: 0.1735 - val_rmse: 0.4166\n",
            "Epoch 251/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1706 - rmse: 0.4131 - val_loss: 0.1699 - val_rmse: 0.4121\n",
            "Epoch 252/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1794 - rmse: 0.4235 - val_loss: 0.1639 - val_rmse: 0.4048\n",
            "Epoch 253/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1776 - rmse: 0.4214 - val_loss: 0.1786 - val_rmse: 0.4227\n",
            "Epoch 254/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1648 - rmse: 0.4060 - val_loss: 0.1736 - val_rmse: 0.4167\n",
            "Epoch 255/400\n",
            "600/600 [==============================] - 24s 41ms/step - loss: 0.1726 - rmse: 0.4154 - val_loss: 0.1639 - val_rmse: 0.4049\n",
            "Epoch 256/400\n",
            "600/600 [==============================] - 25s 43ms/step - loss: 0.1717 - rmse: 0.4143 - val_loss: 0.1641 - val_rmse: 0.4051\n",
            "Epoch 257/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1787 - rmse: 0.4227 - val_loss: 0.1645 - val_rmse: 0.4056\n",
            "Epoch 258/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1749 - rmse: 0.4182 - val_loss: 0.1660 - val_rmse: 0.4074\n",
            "Epoch 259/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1848 - rmse: 0.4299 - val_loss: 0.1646 - val_rmse: 0.4057\n",
            "Epoch 260/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1670 - rmse: 0.4086 - val_loss: 0.1698 - val_rmse: 0.4121\n",
            "Epoch 261/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1727 - rmse: 0.4155 - val_loss: 0.1660 - val_rmse: 0.4074\n",
            "Epoch 262/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1683 - rmse: 0.4103 - val_loss: 0.1648 - val_rmse: 0.4060\n",
            "Epoch 263/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1697 - rmse: 0.4120 - val_loss: 0.1659 - val_rmse: 0.4073\n",
            "Epoch 264/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1714 - rmse: 0.4140 - val_loss: 0.1672 - val_rmse: 0.4090\n",
            "Epoch 265/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1647 - rmse: 0.4059 - val_loss: 0.1689 - val_rmse: 0.4110\n",
            "Epoch 266/400\n",
            "600/600 [==============================] - 27s 46ms/step - loss: 0.1705 - rmse: 0.4129 - val_loss: 0.1637 - val_rmse: 0.4046\n",
            "Epoch 267/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1704 - rmse: 0.4129 - val_loss: 0.1643 - val_rmse: 0.4053\n",
            "Epoch 268/400\n",
            "600/600 [==============================] - 25s 41ms/step - loss: 0.1770 - rmse: 0.4207 - val_loss: 0.1625 - val_rmse: 0.4031\n",
            "Epoch 269/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1760 - rmse: 0.4195 - val_loss: 0.1659 - val_rmse: 0.4074\n",
            "Epoch 270/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1662 - rmse: 0.4077 - val_loss: 0.1832 - val_rmse: 0.4281\n",
            "Epoch 271/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1711 - rmse: 0.4137 - val_loss: 0.1631 - val_rmse: 0.4038\n",
            "Epoch 272/400\n",
            "600/600 [==============================] - 25s 42ms/step - loss: 0.1740 - rmse: 0.4172 - val_loss: 0.1725 - val_rmse: 0.4154\n",
            "Epoch 273/400\n",
            "600/600 [==============================] - 28s 46ms/step - loss: 0.1780 - rmse: 0.4219 - val_loss: 0.1812 - val_rmse: 0.4257\n",
            "Epoch 274/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1746 - rmse: 0.4178 - val_loss: 0.1699 - val_rmse: 0.4122\n",
            "Epoch 275/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1850 - rmse: 0.4301 - val_loss: 0.1729 - val_rmse: 0.4158\n",
            "Epoch 276/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1653 - rmse: 0.4066 - val_loss: 0.1651 - val_rmse: 0.4063\n",
            "Epoch 277/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1712 - rmse: 0.4138 - val_loss: 0.1659 - val_rmse: 0.4073\n",
            "Epoch 278/400\n",
            "600/600 [==============================] - 26s 43ms/step - loss: 0.1694 - rmse: 0.4116 - val_loss: 0.1667 - val_rmse: 0.4083\n",
            "Epoch 279/400\n",
            "600/600 [==============================] - 27s 44ms/step - loss: 0.1713 - rmse: 0.4138 - val_loss: 0.1665 - val_rmse: 0.4080\n",
            "Epoch 280/400\n",
            "600/600 [==============================] - 27s 45ms/step - loss: 0.1684 - rmse: 0.4104 - val_loss: 0.1622 - val_rmse: 0.4028\n",
            "Epoch 281/400\n",
            "600/600 [==============================] - 26s 44ms/step - loss: 0.1640 - rmse: 0.4050 - val_loss: 0.1742 - val_rmse: 0.4174\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0cb05badf0>"
            ]
          },
          "execution_count": 201,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, baseline=None)\n",
        "logdir = os.path.join(f\"logs_{model_name}\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch')\n",
        "\n",
        "save_path = f'./checkpoint_{model_name}' + \"/model-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, \n",
        "                                                         monitor='val_loss', save_best_only=True)\n",
        "\n",
        "model.fit(train_dataset,\n",
        "         steps_per_epoch=600, # you might need to change this\n",
        "         validation_data=test_dataset,\n",
        "         epochs=400,\n",
        "         callbacks=[early_stopping_callback, tensorboard_callback, checkpoint_callback]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "g1ZZskDZ5L2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4081/4081 [==============================] - 27s 7ms/step - loss: 0.1619 - rmse: 0.4023\n",
            "New model saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./model/assets\n"
          ]
        }
      ],
      "source": [
        "import keras.models\n",
        "val_loss = model.evaluate(test_dataset)[0]\n",
        "if val_loss < baseline_val_loss:\n",
        "  print(\"New model saved.\")\n",
        "  keras.models.save_model(model, f'./model_{model_name}', overwrite=True, include_optimizer=True)\n",
        "  # model.save('./denoiser_cnn_log_mel_generator.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeJTsGxCSuhm"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLiTiK8blE0n"
      },
      "outputs": [],
      "source": [
        "def read_audio(filepath, sample_rate, normalize=True):\n",
        "    \"\"\"Read an audio file and return it as a numpy array\"\"\"\n",
        "    audio, sr = librosa.load(filepath, sr=sample_rate)\n",
        "    if normalize:\n",
        "      div_fac = 1 / np.max(np.abs(audio)) / 3.0\n",
        "      audio = audio * div_fac\n",
        "    return audio, sr\n",
        "        \n",
        "def add_noise_to_clean_audio(clean_audio, noise_signal):\n",
        "    \"\"\"Adds noise to an audio sample\"\"\"\n",
        "    if len(clean_audio) >= len(noise_signal):\n",
        "        # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n",
        "        while len(clean_audio) >= len(noise_signal):\n",
        "            noise_signal = np.append(noise_signal, noise_signal)\n",
        "\n",
        "    ## Extract a noise segment from a random location in the noise file\n",
        "    ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n",
        "\n",
        "    noiseSegment = noise_signal[ind: ind + clean_audio.size]\n",
        "\n",
        "    speech_power = np.sum(clean_audio ** 2)\n",
        "    noise_power = np.sum(noiseSegment ** 2)\n",
        "    noisyAudio = clean_audio + np.sqrt(speech_power / noise_power) * noiseSegment\n",
        "    return noisyAudio\n",
        "\n",
        "def play(audio, sample_rate):\n",
        "    ipd.display(ipd.Audio(data=audio, rate=sample_rate))  # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM6ajbBFlx3b"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor:\n",
        "    def __init__(self, audio, *, windowLength, overlap, sample_rate):\n",
        "        self.audio = audio\n",
        "        self.ffT_length = windowLength\n",
        "        self.window_length = windowLength\n",
        "        self.overlap = overlap\n",
        "        self.sample_rate = sample_rate\n",
        "        self.window = scipy.signal.hamming(self.window_length, sym=False)\n",
        "\n",
        "    def get_stft_spectrogram(self):\n",
        "        return librosa.stft(self.audio, n_fft=self.ffT_length, win_length=self.window_length, hop_length=self.overlap,\n",
        "                            window=self.window, center=True)\n",
        "\n",
        "    def get_audio_from_stft_spectrogram(self, stft_features):\n",
        "        return librosa.istft(stft_features, win_length=self.window_length, hop_length=self.overlap,\n",
        "                             window=self.window, center=True)\n",
        "\n",
        "    def get_mel_spectrogram(self):\n",
        "        return librosa.feature.melspectrogram(self.audio, sr=self.sample_rate, power=2.0, pad_mode='reflect',\n",
        "                                           n_fft=self.ffT_length, hop_length=self.overlap, center=True)\n",
        "\n",
        "    def get_audio_from_mel_spectrogram(self, M):\n",
        "        return librosa.feature.inverse.mel_to_audio(M, sr=self.sample_rate, n_fft=self.ffT_length, hop_length=self.overlap,\n",
        "                                             win_length=self.window_length, window=self.window,\n",
        "                                             center=True, pad_mode='reflect', power=2.0, n_iter=32, length=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dcnyvquSoLs"
      },
      "outputs": [],
      "source": [
        "cleanAudio, sr = read_audio(os.path.join(mozilla_basepath, 'test', 'common_voice_en_16526.mp3'), sample_rate=fs)\n",
        "print(\"Min:\", np.min(cleanAudio),\"Max:\",np.max(cleanAudio))\n",
        "ipd.Audio(data=cleanAudio, rate=sr) # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaHe1okPTvV-"
      },
      "outputs": [],
      "source": [
        "noiseAudio, sr = read_audio(os.path.join(UrbanSound8K_basepath, 'test', '7913-3-0-0.wav'), sample_rate=fs)\n",
        "print(\"Min:\", np.min(noiseAudio),\"Max:\",np.max(noiseAudio))\n",
        "ipd.Audio(data=noiseAudio, rate=sr) # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu_eOKRfTHbp"
      },
      "outputs": [],
      "source": [
        "cleanAudioFeatureExtractor = FeatureExtractor(cleanAudio, windowLength=windowLength, overlap=overlap, sample_rate=sr)\n",
        "stft_features = cleanAudioFeatureExtractor.get_stft_spectrogram()\n",
        "stft_features = np.abs(stft_features)\n",
        "print(\"Min:\", np.min(stft_features),\"Max:\",np.max(stft_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHWcmobyTP4E"
      },
      "outputs": [],
      "source": [
        "noisyAudio = add_noise_to_clean_audio(cleanAudio, noiseAudio)\n",
        "ipd.Audio(data=noisyAudio, rate=fs) # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75M29dl3bBeF"
      },
      "outputs": [],
      "source": [
        "def prepare_input_features(stft_features):\n",
        "    # Phase Aware Scaling: To avoid extreme differences (more than\n",
        "    # 45 degree) between the noisy and clean phase, the clean spectral magnitude was encoded as similar to [21]:\n",
        "    noisySTFT = np.concatenate([stft_features[:,0:numSegments-1], stft_features], axis=1)\n",
        "    stftSegments = np.zeros((numFeatures, numSegments , noisySTFT.shape[1] - numSegments + 1))\n",
        "\n",
        "    for index in range(noisySTFT.shape[1] - numSegments + 1):\n",
        "        stftSegments[:,:,index] = noisySTFT[:,index:index + numSegments]\n",
        "    return stftSegments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cjO5-cjTP6t"
      },
      "outputs": [],
      "source": [
        "noiseAudioFeatureExtractor = FeatureExtractor(noisyAudio, windowLength=windowLength, overlap=overlap, sample_rate=sr)\n",
        "noise_stft_features = noiseAudioFeatureExtractor.get_stft_spectrogram()\n",
        "\n",
        "# Paper: Besides, spectral phase was not used in the training phase.\n",
        "# At reconstruction, noisy spectral phase was used instead to\n",
        "# perform in- verse STFT and recover human speech.\n",
        "noisyPhase = np.angle(noise_stft_features)\n",
        "print(noisyPhase.shape)\n",
        "noise_stft_features = np.abs(noise_stft_features)\n",
        "\n",
        "mean = np.mean(noise_stft_features)\n",
        "std = np.std(noise_stft_features)\n",
        "noise_stft_features = (noise_stft_features - mean) / std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3p5PWWkrlE3m"
      },
      "outputs": [],
      "source": [
        "predictors = prepare_input_features(noise_stft_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pSoOw2fTP9N"
      },
      "outputs": [],
      "source": [
        "predictors = np.reshape(predictors, (predictors.shape[0], predictors.shape[1], 1, predictors.shape[2]))\n",
        "predictors = np.transpose(predictors, (3, 0, 1, 2)).astype(np.float32)\n",
        "print('predictors.shape:', predictors.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5sNR4sSTP_1"
      },
      "outputs": [],
      "source": [
        "STFTFullyConvolutional = model.predict(predictors)\n",
        "print(STFTFullyConvolutional.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzCga3PdUVwG"
      },
      "outputs": [],
      "source": [
        "def revert_features_to_audio(features, phase, cleanMean=None, cleanStd=None):\n",
        "    # scale the outpus back to the original range\n",
        "    if cleanMean and cleanStd:\n",
        "        features = cleanStd * features + cleanMean\n",
        "\n",
        "    phase = np.transpose(phase, (1, 0))\n",
        "    features = np.squeeze(features)\n",
        "\n",
        "    # features = librosa.db_to_power(features)\n",
        "    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n",
        "\n",
        "    features = np.transpose(features, (1, 0))\n",
        "    return noiseAudioFeatureExtractor.get_audio_from_stft_spectrogram(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWlUDtPzURlQ"
      },
      "outputs": [],
      "source": [
        "denoisedAudioFullyConvolutional = revert_features_to_audio(STFTFullyConvolutional, noisyPhase, mean, std)\n",
        "print(\"Min:\", np.min(denoisedAudioFullyConvolutional),\"Max:\",np.max(denoisedAudioFullyConvolutional))\n",
        "ipd.Audio(data=denoisedAudioFullyConvolutional, rate=fs) # load a local WAV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvEIBy7EHgeP"
      },
      "outputs": [],
      "source": [
        "# A numeric identifier of the sound class -- Types of noise\n",
        "# 0 = air_conditioner\n",
        "# 1 = car_horn\n",
        "# 2 = children_playing\n",
        "# 3 = dog_bark\n",
        "# 4 = drilling\n",
        "# 5 = engine_idling\n",
        "# 6 = gun_shot\n",
        "# 7 = jackhammer\n",
        "# 8 = siren\n",
        "# 9 = street_music"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv_7ZwWaUW0_"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2, ax3) = plt.subplots(3, 1, sharey=True)\n",
        "\n",
        "ax1.plot(cleanAudio)\n",
        "ax1.set_title(\"Clean Audio\")\n",
        "\n",
        "ax2.plot(noisyAudio)\n",
        "ax2.set_title(\"Noisy Audio\")\n",
        "\n",
        "ax3.plot(denoisedAudioFullyConvolutional)\n",
        "ax3.set_title(\"Denoised Audio\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LM7E91avKA3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "SpeechDenoiserCNN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('mlp')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a7c2bc294fb4a4c68eb1f6998c466cc1127bf5e3b69a7adb6cd789b68ede878a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
